bases:
- "{{ requiredEnv "MYDIR" }}/helmfile/envs.yaml"
---
repositories:
- name: dandy-developer  # https://github.com/DandyDeveloper/charts
  url: "https://dandydeveloper.github.io/charts"
- name: harbor  # https://github.com/goharbor/harbor-helm
  url: "https://helm.goharbor.io"
- name: sentry  # https://github.com/sentry-kubernetes/charts
  url: "https://sentry-kubernetes.github.io/charts"
- name: anchore  # https://github.com/anchore/anchore-charts
  url: "https://charts.anchore.io"
- name: scylla
  url: "https://scylla-operator-charts.storage.googleapis.com/stable"

templates:
  harbor:
    values:
    - component: &harborVersion
        image:
          tag: v{{ .Values.versions.images.harbor }}

{{- $s3 := dict "endpoint" (printf "%s-s3.%s.svc:%s" .Values.releases.seaweedfs .Values.namespaces.storage (toString .Values.seaweedfs.ports.http.s3)) "accessKey" .Values.seaweedfs.s3.keys.admin.accessKey "secretKey" .Values.seaweedfs.s3.keys.admin.secretKey }}

releases:
- name: {{ .Values.releases.seaweedfs }}
  namespace: {{ .Values.namespaces.storage }}
  chart: {{ .Values.chartBases.seaweedfs | quote }}
  installed: {{ .Values.enables.storage }}
  needs:
{{- if ne .Values.k8sRuntime "k3d" }}
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.openebs }}
{{- end }}
  #- {{ .Values.namespaces.storage }}/{{ .Values.releases.patroni }}
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.redis }}
  #- {{ .Values.namespaces.monitoring }}/{{ .Values.releases.kubePromStack }}
  values:
  - global:
      # https://github.com/chrislusf/seaweedfs/wiki/Replication#the-meaning-of-replication-type
      enableReplication: true
      replicationPlacment: "001"
      #enableSecurity: true
    master:
      enabled: true
      replicas: 3
      affinity: ""
      nodeSelector: ""
      port: {{ .Values.seaweedfs.ports.http.master }}
      grpcPort: {{ .Values.seaweedfs.ports.grpc.master }}
      defaultReplication: "001"
      #storage: 25Gi
      #storageClass: null
      extraEnvironmentVars:
        WEED_MASTER_REPLICATION_TREAT_REPLICATION_AS_MINIMUMS: "true"
    cronjob:
      nodeSelector: ""
    volume:
      enabled: true
      replicas: 3
      affinity: ""
      nodeSelector: ""
      port: {{ .Values.seaweedfs.ports.http.volume }}
      grpcPort: {{ .Values.seaweedfs.ports.grpc.volume }}
      fileSizeLimitMB: "1048576"  # 1TiB, if you have a file that big, welpâ€¦
      minFreeSpacePercent: 0
      compactionMBps: "1048576" # 1TiB/s
      index: leveldbLarge
      metricsPort: 0
      dataCenter: dc0
      rack: r0
      dir: /data
      dir_idx: /idx
      maxVolumes: 256
      data:
        type: persistentVolumeClaim
        size: 8Ti
      idx:
        type: persistentVolumeClaim
        size: 8Ti
      logs:
        type: persistentVolumeClaim
        size: 8Ti
    filer:
      enabled: true
      replicas: 3
      affinity: ""
      nodeSelector: ""
      port: {{ .Values.seaweedfs.ports.http.filer }}
      grpcPort: {{ .Values.seaweedfs.ports.grpc.filer }}
      metricsPort: 0
      defaultReplicaPlacement: "001"
      s3:
        allowEmptyFolder: true
        enableAuth: true
        port: {{ .Values.seaweedfs.ports.http.s3 }}
      #  skipAuthSecretCreation: true
      #  enabled: false
      #  domainName: ""
        keys:
          admin:
            accessKey: {{ .Values.seaweedfs.s3.keys.admin.accessKey | quote }}
            secretKey: {{ .Values.seaweedfs.s3.keys.admin.secretKey | quote }}
      extraEnvironmentVars:
{{- if .Values.seaweedfs.filerIndex.redis.enabled }}
        #WEED_REDIS_CLUSTER_ENABLED: "true"
        #WEED_REDIS_CLUSTER_ADDRESSES: '["{{ .Values.releases.redis }}-haproxy.{{ .Values.namespaces.storage }}.svc:{{ .Values.redis.ports.readwrite }}"]'
        #WEED_REDIS_CLUSTER_PASSWORD: {{ .Values.redis.password | quote }}
        #WEED_REDIS_CLUSTER_DATABASE: {{ .Values.seaweedfs.filerIndex.redis.db | quote }}
        #WEED_REDIS_CLUSTER2_SUPERLARGEDIRECTORIES: '["/"]'
        WEED_REDIS_ENABLED: "true"
        WEED_REDIS_ADDRESS: "{{ .Values.releases.redis }}-haproxy.{{ .Values.namespaces.storage }}.svc:{{ .Values.redis.ports.readwrite }}"
        WEED_REDIS_PASSWORD: {{ .Values.redis.password | quote }}
        WEED_REDIS_DATABASE: {{ .Values.seaweedfs.filerIndex.redis.db | quote }}
        #WEED_REDIS2_SUPERLARGEDIRECTORIES: '["/"]'
{{- else if .Values.seaweedfs.filerIndex.postgresql.enabled }}
        WEED_POSTGRES_ENABLED: "true"
        WEED_POSTGRES_SSLMODE: "prefer"
        WEED_POSTGRES_HOSTNAME: "{{ .Values.releases.patroni }}.{{ .Values.namespaces.storage }}.svc"
        WEED_POSTGRES_USERNAME: {{ .Values.seaweedfs.filerIndex.postgresql.user | quote }}
        WEED_POSTGRES_PASSWORD: {{ .Values.seaweedfs.filerIndex.postgresql.pass | quote }}
        WEED_POSTGRES_DATABASE: {{ .Values.seaweedfs.filerIndex.postgresql.name | quote }}
        WEED_POSTGRES_CONNECTION_MAX_IDLE: "5"
        WEED_POSTGRES_CONNECTION_MAX_OPEN: "75"
        WEED_POSTGRES_CONNECTION_MAX_LIFETIME_SECONDS: "600"
{{- else }}
        WHATEVER: smirk
{{- end }}
    s3:
    #  enabled: true
      replicas: 3
      nodeSelector: ""
      port: {{ .Values.seaweedfs.ports.http.s3 }}
      metricsPort: 0
      allowEmptyFolder: true
      enableAuth: true
    #  skipAuthSecretCreation: true
    #  domainName: ""
      logs:
        type: persistentVolumeClaim
        size: 8Ti
      keys:
        admin:
          accessKey: {{ .Values.seaweedfs.s3.keys.admin.accessKey | quote }}
          secretKey: {{ .Values.seaweedfs.s3.keys.admin.secretKey | quote }}
  hooks:
  - events: ["postsync"]
    command: "/bin/sh"
    args:
    - "-xec"
    - |
      MC_NAME="mc-${RANDOM}"
      kubectl -n {{ .Values.namespaces.storage }} apply -f- <<EOF
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: ${MC_NAME}
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: minio-client
            release: ${MC_NAME}
        template:
          metadata:
            labels:
              app: minio-client
              release: ${MC_NAME}
          spec:
            containers:
            - name: mc
              image: minio/mc
              command: ['/bin/sh', '-c', 'while true; do sleep 86400; done']
      EOF
      until kubectl -n {{ .Values.namespaces.storage }} wait --for condition=available deploy ${MC_NAME}; do sleep 1; done 2>/dev/null
      kubectl -n {{ .Values.namespaces.storage }} exec $(kubectl -n {{ .Values.namespaces.storage }} get pods -o jsonpath="{.items[?(@.metadata.labels.release==\"${MC_NAME}\")].metadata.name}" | awk '{print $1}') -- /bin/sh -xc 'until curl http://{{ $s3.endpoint }} &>/dev/null; do sleep 3; done; mc config host add seaweedfs http://{{ $s3.endpoint }} {{ $s3.accessKey | quote }} {{ $s3.secretKey | quote }}; for i in {{ .Values.thanosObjstoreConfig.bucket | quote }} {{ .Values.harbor.bucket | quote }} {{ .Values.sentry.bucket | quote }} {{ .Values.cortex.buckets.blocks | quote }} {{ .Values.cortex.buckets.notify | quote }} {{ .Values.loki.buckets.chunks | quote }} {{ .Values.loki.buckets.notify | quote }} {{ .Values.patroni.walBucket | quote }}; do mc ls "seaweedfs/${i}" || mc mb "seaweedfs/${i}"; done'
      echo "MinIO is available under http://{{ $s3.endpoint }} with access key \"{{ $s3.accessKey }}\" and secret key \"{{ $s3.secretKey }}\"."

{{- $patroniHashedName := printf "%s-%s" .Values.releases.patroni (printf "%s:%s" .Values.patroni.image .Values.patroni.tag | sha256sum | trunc 8) }}
- name: {{ .Values.releases.patroni }}
  namespace: {{ .Values.namespaces.storage }}
  chart: "{{ requiredEnv "MYDIR" }}/charts/zer0def/incubator/patroni"
  installed: {{ .Values.enables.storage }}
  needs:
  - {{ .Values.namespaces.network }}/{{ .Values.releases.certManager }}
{{- if index .Values.patroni "etcd" }}
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.etcd }}
{{- end }}
{{- if index .Values.patroni "walBucket" }}
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.seaweedfs }}
{{- end }}
{{- if ne .Values.k8sRuntime "k3d" }}
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.openebs }}
{{- end }}
  values:
  - fullnameOverride: {{ .Values.releases.patroni | quote }}
    replicaCount: {{ .Values.patroni.replicas }}
    image:
      repository: {{ .Values.patroni.image }}
      tag: {{ .Values.patroni.tag }}
    pgbouncer:
      replicaCount: {{ .Values.patroni.bouncers }}
      tls:
        server:
          issuerRef:
            name: selfsigned-ca
            kind: Issuer
          sslmode: verify-full
        client:
          issuerRef:
            name: selfsigned-ca
            kind: Issuer
    tls:
      issuerRef:
        name: selfsigned-ca
        kind: Issuer
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            topologyKey: "kubernetes.io/hostname"
            #namespaceSelector: {}
            labelSelector:
              matchLabels:
                release: {{ $patroniHashedName }}
    env:
      ALLOW_NOSSL: "true"
{{- if index .Values.patroni "walBucket" }}
      AWS_ENDPOINT: "http://{{ $s3.endpoint }}"
      AWS_REGION: us-east-1
      AWS_ACCESS_KEY_ID: {{ $s3.accessKey | quote }}
      AWS_SECRET_ACCESS_KEY: {{ $s3.secretKey | quote }}
      AWS_S3_FORCE_PATH_STYLE: "true"
      USE_WALG_BACKUP: "true"
      USE_WALG_RESTORE: "true"
      WALG_S3_PREFIX: "s3://{{ .Values.patroni.walBucket }}"
      WALE_S3_PREFIX: "s3://{{ .Values.patroni.walBucket }}"
{{- end }}
{{- if index .Values.patroni "etcd" }}
      ETCD3_HOSTS: '"{{ .Values.releases.etcd }}-headless.{{ .Values.namespaces.storage }}.svc:2379"'
    kubernetes:
      dcs:
        enable: false
{{- end }}
    spiloConfiguration:
      bootstrap:
        dcs:
          synchronous_mode: true
          #synchronous_mode_strict: true
          synchronous_node_count: {{ sub .Values.patroni.replicas 1 }}
        initdb:
        - data-checksums
        - locale: en_US.UTF-8
        - encoding: UTF-8
      postgresql:
        use_slots: true
        remove_data_directory_on_diverged_timelines: true
        #use_pg_rewind: false
        remove_data_directory_on_rewind_failure: true
        parameters:
          log_destination: stderr
          logging_collector: "off"
          shared_preload_libraries: citus,timescaledb
          timescaledb.license: timescale  # it's a trap!
          timescaledb.telemetry_level: "off"
    metrics:
      postgresql:
        enabled: false
        probes:
          liveness:
            enabled: false
          readiness:
            enabled: false
      patroni:
        enabled: false
        probes:
          liveness:
            enabled: false
          readiness:
            enabled: false
    databases:
    - name: {{ .Values.harbor.db.core.name | quote }}
      user: {{ .Values.harbor.db.core.user | quote }}
      pass: {{ .Values.harbor.db.core.pass | quote }}
    - name: {{ .Values.harbor.db.notaryServer.name | quote }}
      user: {{ .Values.harbor.db.core.user | quote }}
      pass: {{ .Values.harbor.db.core.pass | quote }}
      #user: {{ .Values.harbor.db.notaryServer.user | quote }}
      #pass: {{ .Values.harbor.db.notaryServer.pass | quote }}
    - name: {{ .Values.harbor.db.notarySigner.name | quote }}
      user: {{ .Values.harbor.db.core.user | quote }}
      pass: {{ .Values.harbor.db.core.pass | quote }}
      #user: {{ .Values.harbor.db.notarySigner.user | quote }}
      #pass: {{ .Values.harbor.db.notarySigner.pass | quote }}
    - name: {{ .Values.kong.db.name | quote }}
      user: {{ .Values.kong.db.user | quote }}
      pass: {{ .Values.kong.db.pass | quote }}
    - name: {{ .Values.sentry.db.name | quote }}
      user: {{ .Values.sentry.db.user | quote }}
      pass: {{ .Values.sentry.db.pass | quote }}
    - name: {{ .Values.timescale.db.name | quote }}
      user: {{ .Values.timescale.db.user | quote }}
      pass: {{ .Values.timescale.db.pass | quote }}
    - name: {{ .Values.anchore.db.name | quote }}
      user: {{ .Values.anchore.db.user | quote }}
      pass: {{ .Values.anchore.db.pass | quote }}
{{- if .Values.seaweedfs.filerIndex.postgresql.enabled }}
    - name: {{ .Values.seaweedfs.postgresql.name | quote }}
      user: {{ .Values.seaweedfs.postgresql.user | quote }}
      pass: {{ .Values.seaweedfs.postgresql.pass | quote }}
{{- end }}

- name: {{ .Values.releases.cassandra }}-operator
  namespace: {{ .Values.namespaces.storage }}
  #chart: scylla/scylla-operator
  chart: "{{ .Values.chartBases.scylla }}/scylla-operator"
  installed: {{ .Values.enables.storage }}
  needs:
{{- if ne .Values.k8sRuntime "k3d" }}
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.openebs }}
{{- end }}
  hooks:
  - events: ["postsync"]
    command: "/bin/sh"
    args:
    - "-xec"
    - |
      until kubectl wait --for condition=established crd/scyllaclusters.scylla.scylladb.com; do sleep 1; done 2>/dev/null ||:
      sleep 30
  values:
  - fullnameOverride: "{{ .Values.releases.cassandra }}-operator"
    psp:
      enabled: true
    scyllaClusters:
    - name: {{ .Values.releases.cassandra }}
      namespace: {{ .Values.namespaces.storage }}
      scyllaImage:
        tag: {{ .Values.versions.images.scyllaServer }}
      agentImage:
        tag: {{ .Values.versions.images.scyllaAgent }}
      alternator:
        enabled: true
      developerMode: true
      sysctls:
      - fs.aio-max-nr=2097152
      serviceMonitor:  # parameterize
        create: false
      racks:
      - {}

- name: {{ .Values.releases.cassandra }}-manager
  namespace: {{ .Values.namespaces.storage }}
  #chart: scylla/scylla-manager
  chart: "{{ .Values.chartBases.scylla }}/scylla-manager"
  #installed: {{ .Values.enables.storage }}
  installed: false
  needs:
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.cassandra }}-operator
{{- if ne .Values.k8sRuntime "k3d" }}
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.openebs }}
{{- end }}
  values:
  - fullnameOverride: "{{ .Values.releases.cassandra }}-manager"
    image:
      tag: {{ .Values.versions.images.scyllaManager }}
    logLevel: trace
    serviceMonitor:
      create: false
    scylla:
      scyllaImage:
        tag: {{ .Values.versions.images.scyllaServer }}
      agentImage:
        tag: {{ .Values.versions.images.scyllaAgent }}
      alternator:
        enabled: true
      developerMode: true
      serviceMonitor:  # parameterize
        create: false

- name: {{ .Values.releases.cassandra }}
  namespace: {{ .Values.namespaces.storage }}
  #chart: scylla/scylla
  chart: "{{ .Values.chartBases.scylla }}/scylla"
  #installed: {{ .Values.enables.storage }}
  installed: false
  needs:
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.cassandra }}-operator
{{- if ne .Values.k8sRuntime "k3d" }}
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.openebs }}
{{- end }}
  values:
  - fullnameOverride: {{ .Values.releases.cassandra }}
    scyllaImage:
      tag: {{ .Values.versions.images.scyllaServer }}
    agentImage:
      tag: {{ .Values.versions.images.scyllaAgent }}
    alternator:
      enabled: true
    developerMode: true
    serviceMonitor:
      create: false

- name: {{ .Values.releases.redis }}
  namespace: {{ .Values.namespaces.storage }}
  chart: dandy-developer/redis-ha
  installed: {{ .Values.enables.storage }}
{{- if ne .Values.k8sRuntime "k3d" }}
  needs:
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.openebs }}
{{- end }}
  values:
  - fullnameOverride: {{ .Values.releases.redis | quote }}
    hardAntiAffinity: false
    replicas: {{ .Values.redis.replicas.redis }}
    image:
      tag: {{ .Values.versions.images.redis }}
    haproxy:  # required for harbor-jobservice to do proper init, but also generally a decent idea
      enabled: true
      replicas: {{ .Values.redis.replicas.haproxy }}
      stickyBalancing: true
      hardAntiAffinity: false
      readOnly:
        enabled: true
        port: {{ .Values.redis.ports.readonly }}
      metrics:
        enabled: true
      image:
        tag: {{ .Values.versions.images.redisHaproxy }}
    redis:
      port: {{ .Values.redis.ports.readwrite }}
      config:
        databases: {{ .Values.redis.databaseCount }}
    sentinel:
      port: {{ .Values.redis.ports.sentinel }}
    exporter:
      enabled: true
      tag: alpine
      extraArgs:  # ??
        ping-on-connect: true
        redis-only-metrics: true
      serviceMonitor:
        enabled: true
    auth: true
    redisPassword: {{ .Values.redis.password }}

- name: {{ .Values.releases.anchore }}
  namespace: {{ .Values.namespaces.storage }}
  chart: anchore/anchore-engine
  #installed: {{ .Values.enables.storage }}
  installed: {{ .Values.enables.anchore }}
  needs:
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.patroni }}
  values:
  - postgresql:
      enabled: false
      externalEndpoint: "{{ .Values.releases.patroni }}.{{ .Values.namespaces.storage }}.svc:5432"
      postgresUser: {{ .Values.anchore.db.user }}
      postgresPassword: {{ .Values.anchore.db.pass }}
      postgresDatabase: {{ .Values.anchore.db.name }}
    anchore-feeds-db:
      enabled: false
    anchoreEnterpriseFeeds:
      enabled: false
    anchoreEnterpriseFeedsUpgradeJob:
      enabled: false
    anchoreEnterpriseRbac:
      enabled: false
    anchoreEnterpriseReports:
      enabled: false
    anchoreEnterpriseUi:
      enabled: false
    anchore-ui-redis:
      enabled: false

- name: {{ .Values.releases.harbor }}
  namespace: {{ .Values.namespaces.storage }}
  chart: harbor/harbor
  version: {{ .Values.versions.charts.harbor }}
  installed: {{ .Values.enables.storage }}
  needs:
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.patroni }}
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.redis }}
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.seaweedfs }}
{{- if .Values.enables.anchore }}
  hooks:
  - events: ["postsync"]
    command: "/bin/sh"
    args:
    - "-xec"
    - |
      # https://github.com/anchore/harbor-scanner-adapter
      kubectl -n {{ .Values.namespaces.storage }} apply -f- <<EOF
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: harbor-scanner-anchore
        labels:
          app: harbor-scanner-anchore
      spec:
        selector:
          matchLabels:
            app: harbor-scanner-anchore
        replicas: 1
        template:
          metadata:
            labels:
              app: harbor-scanner-anchore
          spec:
            containers:
            - name: adapter
              image: anchore/harbor-scanner-adapter:1.0.1
              imagePullPolicy: IfNotPresent
              env:
              - name: SCANNER_ADAPTER_LISTEN_ADDR
                value: ":8080"
              - name: ANCHORE_ENDPOINT
                value: "http://anchore-anchore-engine-api:8228"
              - name: ANCHORE_USERNAME
                value: {{ default "admin" (index .Values.anchore "admin_username") }}
              - name: ANCHORE_PASSWORD
                valueFrom:
                  secretKeyRef:
                    name: {{ .Values.releases.anchore }}-anchore-engine-admin-pass
                    key: ANCHORE_ADMIN_PASSWORD
              - name: ANCHORE_CLIENT_TIMEOUT_SECONDS
                value: "60"
              - name: SCANNER_ADAPTER_FILTER_VENDOR_IGNORED
                value: "true"
              - name: SCANNER_ADAPTER_LOG_LEVEL
                value: "debug"
              - name: SCANNER_ADAPTER_REGISTRY_TLS_VERIFY
                value: "false"
      ---
      apiVersion: v1
      kind: Service
      metadata:
        name: harbor-scanner-anchore
      spec:
        selector:
          app: harbor-scanner-anchore
        type: ClusterIP
        ports:
        - protocol: TCP
          port: 8080
          targetPort: 8080
      EOF
{{- end }}
  values:
  - nameOverride: {{ .Values.releases.harbor | quote }}
    # default admin username for Harbor is `admin`
    harborAdminPassword: {{ .Values.harbor.adminPassword | quote }}
    externalURL: "https://{{ .Values.harbor.coreHostname }}"
    #externalURL: "https://{{ .Values.releases.harbor }}-{{ .Values.releases.harbor }}-core.{{ .Values.namespaces.storage }}.svc"
    expose:  # by default, service is `ingress`
      tls:
        enabled: true
      #  auto:
      #    commonName: {{ .Values.harbor.coreHostname }}
      #type: nodePort
      #nodePort:
      #  ports:
      #    http:
      #      nodePort: {{ .Values.harbor.httpNodePort }}
      ingress:
        annotations:  # be sure to suffix registry with `:80` when pushing images
          nginx.org/client-max-body-size: "0"
          ingress.kubernetes.io/ssl-redirect: "false"
          nginx.ingress.kubernetes.io/ssl-redirect: "false"
        hosts:
          core: {{ .Values.harbor.coreHostname }}
    persistence:
      imageChartStorage:
        disableredirect: true
        type: s3
        s3:
          accesskey: {{ $s3.accessKey | quote }}
          secretkey: {{ $s3.secretKey | quote }}
          bucket: {{ .Values.harbor.bucket | quote }}
          # if using insecure object storage endpoints, you need to prefix it with `http://` protocol for chartmuseum to not barf
          secure: false
          regionendpoint: {{ $s3.endpoint | quote }}
    database:
      type: external
      external:
        host: {{ .Values.releases.patroni }}.{{ .Values.namespaces.storage }}.svc
        username: {{ .Values.harbor.db.core.user | quote }}
        password: {{ .Values.harbor.db.core.pass | quote }}
        coreDatabase: {{ .Values.harbor.db.core.name | quote }}
        notaryServerDatabase: {{ .Values.harbor.db.notaryServer.name | quote }}
        notarySignerDatabase: {{ .Values.harbor.db.notarySigner.name | quote }}
        #sslmode: require
    redis:
      type: external
      external:
        addr: "{{ .Values.releases.redis }}-haproxy.{{ .Values.namespaces.storage }}.svc:{{ .Values.redis.ports.readwrite }}"
        #host: {{ .Values.releases.redis }}-haproxy.{{ .Values.namespaces.storage }}.svc
        password: {{ .Values.redis.password | quote }}
        coreDatabaseIndex: "0"
        jobserviceDatabaseIndex: "1"
        registryDatabaseIndex: "2"
        chartmuseumDatabaseIndex: "3"
        trivyAdapterIndex: "4"
    nginx:
      <<: *harborVersion
    portal:
      <<: *harborVersion
    core:
      <<: *harborVersion
    jobservice:
      <<: *harborVersion
      jobLogger:
      - database
      #- file
      #- stdout
    registry:
      registry:
        <<: *harborVersion
      controller:
        <<: *harborVersion
    chartmuseum:
      <<: *harborVersion
    trivy:
      <<: *harborVersion
    notary:
      server:
        <<: *harborVersion
      signer:
        <<: *harborVersion

- name: {{ .Values.releases.kafka }}
  namespace: {{ .Values.namespaces.storage }}
  installed: {{ .Values.enables.storage }}
  chart: "{{ requiredEnv "MYDIR" }}/charts/redpanda/redpanda"
  values:
  - fullnameOverride: {{ .Values.releases.kafka }}
    image:
      tag: "v{{ .Values.versions.images.redPanda }}"
    statefulset:
      replicas: {{ .Values.kafka.replicas }}
    config:  # https://vectorized.io/docs/configuration/
      redpanda:
        admin:
          address: "0.0.0.0"
        rpc_server:
          address: "0.0.0.0"

- name: {{ .Values.releases.clickhouse }}
  namespace: {{ .Values.namespaces.storage }}
  chart: "{{ .Values.chartBases.sentry }}/clickhouse"
  installed: {{ .Values.enables.storage }}
  values:
  - clickhouse:
      replicas: {{ .Values.clickhouse.replicas | quote }}
      image: {{ .Values.clickhouse.image }}
      imageVersion: {{ .Values.versions.images.clickhouse }}
      http_port: {{ .Values.clickhouse.httpPort | quote }}
      tcp_port: {{ .Values.clickhouse.tcpPort | quote }}
      configmap:
        remote_servers:
          internal_replication: true
          replica:
            backup:
              enabled: false
        users:
          enabled: true
          user:
          - name: {{ .Values.clickhouse.firstUser.username }}
            config:
              password: {{ .Values.clickhouse.firstUser.password }}
              networks:
              - ::/0
              profile: default
              quota: default

- name: {{ .Values.releases.sentry }}
  namespace: {{ .Values.namespaces.storage }}
  installed: {{ .Values.enables.storage }}
  chart: "{{ .Values.chartBases.sentry }}/sentry"
  skipDeps: true
  needs:
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.clickhouse }}
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.kafka }}
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.patroni }}
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.redis }}
  values:
  - sentry:
      worker:
        replicas: 1
    images:
      sentry:
        tag: {{ .Values.versions.images.sentry }}
    user:
      email: {{ .Values.sentry.admin.user }}
      password: {{ .Values.sentry.admin.password }}
    postgresql:
      enabled: false
    externalPostgresql:
      host: {{ .Values.releases.patroni }}.{{ .Values.namespaces.storage }}.svc
      username: {{ .Values.sentry.db.user | quote }}
      password: {{ .Values.sentry.db.pass | quote }}
      database: {{ .Values.sentry.db.name | quote }}
    redis:
      enabled: false
    externalRedis:
      host: {{ .Values.releases.redis }}-haproxy.{{ .Values.namespaces.storage }}.svc
      password: {{ .Values.redis.password | quote }}
    clickhouse:
      enabled: false
      clickhouse:
        image: {{ .Values.clickhouse.image }}
        imageVersion: {{ .Values.versions.images.clickhouse }}
    externalClickhouse:
      host: {{ .Values.releases.clickhouse }}.{{ .Values.namespaces.storage }}.svc
      tcpPort: {{ .Values.clickhouse.tcpPort }}
      httpPort: {{ .Values.clickhouse.httpPort }}
      username: {{ .Values.clickhouse.firstUser.username }}
      password: {{ .Values.clickhouse.firstUser.password }}
      clusterName: {{ .Values.releases.clickhouse }}
    kafka:
      enabled: false
    externalKafka:
      host: "{{ .Values.releases.kafka }}.{{ .Values.namespaces.storage }}.svc"
      port: 9092
{{/*
{{- range $i, $v := until .Values.kafka.replicas }}
    - host: "{{ $.Values.releases.kafka }}-{{ $i }}.{{ $.Values.releases.kafka }}.{{ $.Values.namespaces.storage }}.svc"
      port: 9092
{{- end }}
*/}}
    nginx:
      enabled: false
    rabbitmq:
      enabled: false
    ingress:
      enabled: true
    relay:
      mode: proxy
    filestore:
      backend: s3
      s3:
        signature_version: v2
        accessKey: {{ $s3.accessKey | quote }}
        secretKey: {{ $s3.secretKey | quote }}
        bucketName: {{ .Values.sentry.bucket | quote }}
        endpointUrl: {{ $s3.endpoint | quote }}
    snuba:
{{- range (list "api" "dbInitJob" "migrateJob" "consumer" "replacer" "outcomesConsumer" "sessionsConsumer" "transactionsConsumer" "cleanupErrors" "cleanupTransactions") }}
      {{ . }}:
        replicas: 1
        env:
        - name: CLICKHOUSE_PORT
          value: {{ $.Values.clickhouse.tcpPort | quote }}
{{- end }}
  hooks:
  - events: ["presync"]
    command: "/bin/sh"
    args:
    - "-xec"
    - |
{{- range $i, $v := until .Values.kafka.replicas }}
      until kubectl -n {{ $.Values.namespaces.storage }} wait --for condition=ready pod {{ $.Values.releases.kafka }}-{{ $i }}; do sleep 1; done 2>/dev/null
{{- end }}

- name: swift
  namespace: {{ .Values.namespaces.storage }}
  #installed: {{ .Values.enables.storage }}
  installed: false
  chart: "{{ .Values.chartBases.swift }}/swift"
{{- if ne .Values.k8sRuntime "k3d" }}
  needs:
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.openebs }}
{{- end }}
  values:
  - debug: true
    alerts:
      enabled: false
    image:
      repository: zer0def/docker-swift
      tag: latest
    haproxy:
      image:
        repository: haproxy
        tag: lts-alpine
