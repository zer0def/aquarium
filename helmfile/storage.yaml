bases:
- "{{ requiredEnv "MYDIR" }}/helmfile/envs.yaml"
---
repositories:
- name: dandy-developer  # redis, https://github.com/DandyDeveloper/charts
  url: "https://dandydeveloper.github.io/charts"
- name: scylla
  url: "https://scylla-operator-charts.storage.googleapis.com/stable"
- name: yugabyte  # https://github.com/yugabyte/charts
  url: "https://charts.yugabyte.com"
- name: enapter  # keydb, redis fork; https://github.com/enapter/charts
  url: "https://enapter.github.io/charts"

templates:
  harbor:
    values:
    - component: &harborVersion
        image:
          tag: v{{ .Values.versions.images.harbor }}

{{- $s3 := dict "endpoint" (printf "%s-s3.%s.svc:%s" .Values.releases.seaweedfs .Values.namespaces.storage (toString .Values.seaweedfs.ports.http.s3)) "accessKey" .Values.seaweedfs.s3.keys.admin.accessKey "secretKey" .Values.seaweedfs.s3.keys.admin.secretKey }}

releases:
- name: {{ .Values.releases.seaweedfs }}
  namespace: {{ .Values.namespaces.storage }}
  chart: {{ .Values.chartBases.seaweedfs | quote }}
  installed: {{ .Values.enables.storage }}
  needs:
{{- if ne .Values.k8sRuntime "k3d" }}
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.openebs }}
{{- end }}
  #- {{ .Values.namespaces.storage }}/{{ .Values.releases.patroni }}
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.redis }}
  #- {{ .Values.namespaces.monitoring }}/{{ .Values.releases.kubePromStack }}
  values:
  - global:
      # https://github.com/chrislusf/seaweedfs/wiki/Replication#the-meaning-of-replication-type
      enableReplication: true
      replicationPlacment: "001"
      #enableSecurity: true
    master:
      enabled: true
      imageTag: {{ .Values.versions.images.seaweedfs }}
      replicas: 3
      affinity: ""
      nodeSelector: ""
      port: {{ .Values.seaweedfs.ports.http.master }}
      grpcPort: {{ .Values.seaweedfs.ports.grpc.master }}
      defaultReplication: "001"
      volumePreallocate: false
      volumeSizeLimitMB: "30000"
      #storage: 25Gi
      #storageClass: null
      extraEnvironmentVars:
        WEED_MASTER_REPLICATION_TREAT_REPLICATION_AS_MINIMUMS: "true"
    cronjob:
      nodeSelector: ""
    volume:
      enabled: true
      imageTag: {{ .Values.versions.images.seaweedfs }}
      replicas: 3
      affinity: ""
      nodeSelector: ""
      port: {{ .Values.seaweedfs.ports.http.volume }}
      grpcPort: {{ .Values.seaweedfs.ports.grpc.volume }}
      fileSizeLimitMB: "1048576"  # 1TiB, if you have a file that big, welpâ€¦
      minFreeSpacePercent: 0
      compactionMBps: "1048576" # 1TiB/s
      index: leveldbLarge
      metricsPort: 0
      dataCenter: dc0
      rack: r0
      dir: /data
      dir_idx: /idx
      maxVolumes: 256
      data:
        type: persistentVolumeClaim
        size: 8Ti
      idx:
        type: persistentVolumeClaim
        size: 8Ti
      logs:
        type: persistentVolumeClaim
        size: 8Ti
    filer:
      enabled: true
      imageTag: {{ .Values.versions.images.seaweedfs }}
      replicas: 3
      affinity: ""
      nodeSelector: ""
      port: {{ .Values.seaweedfs.ports.http.filer }}
      grpcPort: {{ .Values.seaweedfs.ports.grpc.filer }}
      #metricsPort: 0
      defaultReplicaPlacement: "001"
      s3:
        allowEmptyFolder: true
        enableAuth: true
        port: {{ .Values.seaweedfs.ports.http.s3 }}
      #  skipAuthSecretCreation: true
      #  enabled: false
      #  domainName: ""
        keys:
          admin:
            accessKey: {{ $s3.accessKey | quote }}
            secretKey: {{ $s3.secretKey | quote }}
      extraEnvironmentVars:
{{- if .Values.seaweedfs.filerIndex.redis.enabled }}
        #WEED_REDIS_CLUSTER_ENABLED: "true"
        #WEED_REDIS_CLUSTER_ADDRESSES: '["{{ .Values.releases.redis }}.{{ .Values.namespaces.storage }}.svc:{{ .Values.redis.ports.readwrite }}"]'
        #WEED_REDIS_CLUSTER_PASSWORD: {{ .Values.redis.password | quote }}
        #WEED_REDIS_CLUSTER_DATABASE: {{ .Values.seaweedfs.filerIndex.redis.db | quote }}
        #WEED_REDIS_CLUSTER2_SUPERLARGEDIRECTORIES: '["/"]'
        WEED_REDIS_ENABLED: "true"
        WEED_REDIS_ADDRESS: "{{ .Values.releases.redis }}.{{ .Values.namespaces.storage }}.svc:{{ .Values.redis.ports.readwrite }}"
        WEED_REDIS_PASSWORD: {{ .Values.redis.password | quote }}
        WEED_REDIS_DATABASE: {{ .Values.seaweedfs.filerIndex.redis.db | quote }}
        #WEED_REDIS2_SUPERLARGEDIRECTORIES: '["/"]'
{{- else if .Values.seaweedfs.filerIndex.postgresql.enabled }}
        WEED_POSTGRES_ENABLED: "true"
        WEED_POSTGRES_SSLMODE: "prefer"
        WEED_POSTGRES_HOSTNAME: "{{ .Values.releases.patroni }}.{{ .Values.namespaces.storage }}.svc"
        WEED_POSTGRES_USERNAME: {{ .Values.seaweedfs.filerIndex.postgresql.user | quote }}
        WEED_POSTGRES_PASSWORD: {{ .Values.seaweedfs.filerIndex.postgresql.pass | quote }}
        WEED_POSTGRES_DATABASE: {{ .Values.seaweedfs.filerIndex.postgresql.name | quote }}
        WEED_POSTGRES_CONNECTION_MAX_IDLE: "5"
        WEED_POSTGRES_CONNECTION_MAX_OPEN: "75"
        WEED_POSTGRES_CONNECTION_MAX_LIFETIME_SECONDS: "600"
{{- else }}
        WHATEVER: smirk
{{- end }}
    s3:
    #  enabled: true
      imageTag: {{ .Values.versions.images.seaweedfs }}
      replicas: 3
      nodeSelector: ""
      port: {{ .Values.seaweedfs.ports.http.s3 }}
      metricsPort: 0
      allowEmptyFolder: true
      enableAuth: true
    #  skipAuthSecretCreation: true
    #  domainName: ""
      logs:
        type: persistentVolumeClaim
        size: 8Ti
      keys:
        admin:
          accessKey: {{ $s3.accessKey | quote }}
          secretKey: {{ $s3.secretKey | quote }}
  hooks:
  - events: ["postsync"]
    command: "/bin/sh"
    args:
    - "-xec"
    - |
      MC_NAME="mc-${RANDOM}"
      kubectl -n {{ .Values.namespaces.storage }} apply -f- <<EOF
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: ${MC_NAME}
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: minio-client
            release: ${MC_NAME}
        template:
          metadata:
            labels:
              app: minio-client
              release: ${MC_NAME}
          spec:
            containers:
            - name: mc
              image: minio/mc
              command: ['/bin/sh', '-c', 'while true; do sleep 86400; done']
      EOF
      until kubectl -n {{ .Values.namespaces.storage }} wait --for condition=available deploy ${MC_NAME}; do sleep 1; done 2>/dev/null
      kubectl -n {{ .Values.namespaces.storage }} exec -i $(kubectl -n {{ .Values.namespaces.storage }} get pods -o jsonpath="{.items[?(@.metadata.labels.release==\"${MC_NAME}\")].metadata.name}" | awk '{print $1}') -- /bin/sh -ex <<'EOF'
      until curl http://{{ $s3.endpoint }} &>/dev/null; do sleep 3; done
      mc config host add seaweedfs http://{{ $s3.endpoint }} {{ $s3.accessKey | quote }} {{ $s3.secretKey | quote }}
      for i in {{ .Values.thanosObjstoreConfig.bucket }} {{ .Values.harbor.bucket }} {{ .Values.sentry.bucket }} {{ .Values.cortex.buckets.blocks }} {{ .Values.cortex.buckets.notify }} {{ .Values.loki.buckets.chunks }} {{ .Values.loki.buckets.notify }} {{ .Values.patroni.walBucket }} {{ .Values.gitlab.buckets.registry }} {{ .Values.osp.bucket }} {{ .Values.kafka.bucket }} {{ .Values.skydive.bucket }}; do
        mc ls "seaweedfs/${i}" || mc mb "seaweedfs/${i}"
      done
      EOF
      echo "MinIO is available under http://{{ $s3.endpoint }} with access key \"{{ $s3.accessKey }}\" and secret key \"{{ $s3.secretKey }}\"."

{{- $patroniHashedName := printf "%s-%s" .Values.releases.patroni (printf "%s:%s" .Values.patroni.image .Values.patroni.tag | sha256sum | trunc 8) }}
- name: {{ .Values.releases.patroni }}
  namespace: {{ .Values.namespaces.storage }}
  chart: "{{ requiredEnv "MYDIR" }}/charts/zer0def/patroni"
  installed: {{ .Values.enables.storage }}
  needs:
  #- {{ .Values.namespaces.network }}/{{ .Values.releases.certManager }}
{{- if index .Values.patroni "etcd" }}
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.etcd }}
{{- end }}
{{- if index .Values.patroni "walBucket" }}
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.seaweedfs }}
{{- end }}
{{- if ne .Values.k8sRuntime "k3d" }}
  #- {{ .Values.namespaces.storage }}/{{ .Values.releases.openebs }}
{{- end }}
  values:
  - fullnameOverride: {{ .Values.releases.patroni | quote }}
    replicaCount: {{ .Values.patroni.replicas }}
    image:
      repository: {{ .Values.patroni.image }}
      tag: {{ .Values.patroni.tag }}
    pgbouncer:
      replicaCount: {{ .Values.patroni.bouncers }}
      tls:
        server:
          issuerRef:
            name: selfsigned-ca
            kind: Issuer
          sslmode: verify-full
        client:
          issuerRef:
            name: selfsigned-ca
            kind: Issuer
    tls:
      issuerRef:
        name: selfsigned-ca
        kind: Issuer
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            topologyKey: "kubernetes.io/hostname"
            #namespaceSelector: {}
            labelSelector:
              matchLabels:
                release: {{ $patroniHashedName }}
    env:
      ALLOW_NOSSL: "true"
{{- if index .Values.patroni "walBucket" }}
      AWS_ENDPOINT: "http://{{ $s3.endpoint }}"
      AWS_REGION: us-east-1
      AWS_ACCESS_KEY_ID: {{ $s3.accessKey | quote }}
      AWS_SECRET_ACCESS_KEY: {{ $s3.secretKey | quote }}
      AWS_S3_FORCE_PATH_STYLE: "true"
      USE_WALG_BACKUP: "true"
      USE_WALG_RESTORE: "true"
      WALG_S3_PREFIX: "s3://{{ .Values.patroni.walBucket }}"
      WALE_S3_PREFIX: "s3://{{ .Values.patroni.walBucket }}"
{{- end }}
{{- if index .Values.patroni "etcd" }}
      ETCD3_HOSTS: '"{{ .Values.releases.etcd }}-headless.{{ .Values.namespaces.storage }}.svc:2379"'
    kubernetes:
      dcs:
        enable: false
{{- end }}
    extraContainers:
    - name: ferretdb
      image: ghcr.io/ferretdb/ferretdb:latest
      env:
      #- name: FERRETDB_LISTEN_UNIX
      #- name: FERRETDB_LISTEN_TLS{,_{CA,CERT,KEY}_FILE}
      - name: FERRETDB_LISTEN_ADDR
        value: ":27017"
      - name: FERRETDB_POSTGRESQL_URL
        value: "postgres://{{ .Values.ferretdb.user }}:{{ .Values.ferretdb.pass }}@{{ .Values.releases.patroni }}.{{ .Values.namespaces.storage }}.svc:5432/{{ .Values.ferretdb.name }}"
      - name: FERRETDB_TELEMETRY
        value: disable
      #args: ["--listen-addr=:27017", "--telemetry=disable", "--postgresql-url=postgres://{{ .Values.ferretdb.user }}:{{ .Values.ferretdb.pass }}@{{ .Values.releases.patroni }}.{{ .Values.namespaces.storage }}.svc:5432/{{ .Values.ferretdb.name }}"]
    extraSvcPorts:
    - name: mongodb
      port: 27017
    spiloConfiguration:
      bootstrap:
        dcs:
          synchronous_mode: true
          #synchronous_mode_strict: true
          synchronous_node_count: {{ sub .Values.patroni.replicas 1 }}
        initdb:
        - data-checksums
        - locale: en_US.UTF-8
        - encoding: UTF-8
      postgresql:
        use_slots: true
        remove_data_directory_on_diverged_timelines: true
        #use_pg_rewind: false
        remove_data_directory_on_rewind_failure: true
        parameters:
          log_destination: stderr
          logging_collector: "off"
          password_encryption: md5  # not particularly safe, but Harbor's not exactly bright either: https://github.com/goharbor/harbor/issues/16135
          shared_preload_libraries: citus,timescaledb
          timescaledb.license: timescale  # it's a trap!
          timescaledb.telemetry_level: "off"
    metrics:
      postgresql:
        enabled: false
        probes:
          liveness:
            enabled: false
          readiness:
            enabled: false
      patroni:
        enabled: false
        probes:
          liveness:
            enabled: false
          readiness:
            enabled: false
    databases:
    - name: {{ .Values.harbor.db.core.name | quote }}
      user: {{ .Values.harbor.db.core.user | quote }}
      pass: {{ .Values.harbor.db.core.pass | quote }}
    - name: {{ .Values.harbor.db.notaryServer.name | quote }}
      user: {{ .Values.harbor.db.core.user | quote }}
      pass: {{ .Values.harbor.db.core.pass | quote }}
      #user: {{ .Values.harbor.db.notaryServer.user | quote }}
      #pass: {{ .Values.harbor.db.notaryServer.pass | quote }}
    - name: {{ .Values.harbor.db.notarySigner.name | quote }}
      user: {{ .Values.harbor.db.core.user | quote }}
      pass: {{ .Values.harbor.db.core.pass | quote }}
      #user: {{ .Values.harbor.db.notarySigner.user | quote }}
      #pass: {{ .Values.harbor.db.notarySigner.pass | quote }}
    - name: {{ .Values.kong.db.name | quote }}
      user: {{ .Values.kong.db.user | quote }}
      pass: {{ .Values.kong.db.pass | quote }}
    - name: {{ .Values.sentry.db.name | quote }}
      user: {{ .Values.sentry.db.user | quote }}
      pass: {{ .Values.sentry.db.pass | quote }}
    - name: {{ .Values.timescale.db.name | quote }}
      user: {{ .Values.timescale.db.user | quote }}
      pass: {{ .Values.timescale.db.pass | quote }}
    - name: {{ .Values.anchore.db.name | quote }}
      user: {{ .Values.anchore.db.user | quote }}
      pass: {{ .Values.anchore.db.pass | quote }}
    - name: {{ .Values.gitlab.pgsql.name | quote }}
      user: {{ .Values.gitlab.pgsql.user | quote }}
      pass: {{ .Values.gitlab.pgsql.pass | quote }}
    - name: {{ .Values.ejabberd.db.name | quote }}
      user: {{ .Values.ejabberd.db.user | quote }}
      pass: {{ .Values.ejabberd.db.pass | quote }}
    - name: {{ .Values.osp.db.name | quote }}
      user: {{ .Values.osp.db.user | quote }}
      pass: {{ .Values.osp.db.pass | quote }}
    - name: {{ .Values.zulip.db.name | quote }}
      user: {{ .Values.zulip.db.user | quote }}
      pass: {{ .Values.zulip.db.pass | quote }}
    - name: {{ .Values.ferretdb.name | quote }}
      user: {{ .Values.ferretdb.user | quote }}
      pass: {{ .Values.ferretdb.pass | quote }}
{{- if .Values.seaweedfs.filerIndex.postgresql.enabled }}
    - name: {{ .Values.seaweedfs.postgresql.name | quote }}
      user: {{ .Values.seaweedfs.postgresql.user | quote }}
      pass: {{ .Values.seaweedfs.postgresql.pass | quote }}
{{- end }}

- name: {{ .Values.releases.cassandra }}-operator
  namespace: {{ .Values.namespaces.storage }}
  #chart: scylla/scylla-operator
  chart: "{{ .Values.chartBases.scylla }}/scylla-operator"
  installed: {{ .Values.enables.storage }}
{{- if ne .Values.k8sRuntime "k3d" }}
  needs:
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.openebs }}
{{- end }}
  hooks:
  - events: ["postsync"]
    command: "/bin/sh"
    args:
    - "-xec"
    - |
      until kubectl wait --for condition=established crd/scyllaclusters.scylla.scylladb.com; do sleep 1; done 2>/dev/null ||:
      sleep 30
  values:
  - fullnameOverride: "{{ .Values.releases.cassandra }}-operator"
    scyllaClusters:
    - name: {{ .Values.releases.cassandra }}
      namespace: {{ .Values.namespaces.storage }}
      scyllaImage:
        tag: {{ .Values.versions.images.scyllaServer }}
      agentImage:
        tag: {{ .Values.versions.images.scyllaAgent }}
      alternator:
        enabled: true
      developerMode: true
      sysctls:
      - fs.aio-max-nr=2097152
      serviceMonitor:  # parameterize
        create: false
      racks:
      - members: 2

- name: {{ .Values.releases.cassandra }}-manager
  namespace: {{ .Values.namespaces.storage }}
  #chart: scylla/scylla-manager
  chart: "{{ .Values.chartBases.scylla }}/scylla-manager"
  #installed: {{ .Values.enables.storage }}
  installed: false
  needs:
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.cassandra }}-operator
{{- if ne .Values.k8sRuntime "k3d" }}
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.openebs }}
{{- end }}
  values:
  - fullnameOverride: "{{ .Values.releases.cassandra }}-manager"
    image:
      tag: {{ .Values.versions.images.scyllaManager }}
    logLevel: trace
    serviceMonitor:
      create: false
    scylla:
      scyllaImage:
        tag: {{ .Values.versions.images.scyllaServer }}
      agentImage:
        tag: {{ .Values.versions.images.scyllaAgent }}
      alternator:
        enabled: true
      developerMode: true
      serviceMonitor:  # parameterize
        create: false

- name: {{ .Values.releases.cassandra }}
  namespace: {{ .Values.namespaces.storage }}
  #chart: scylla/scylla
  chart: "{{ .Values.chartBases.scylla }}/scylla"
  #installed: {{ .Values.enables.storage }}
  installed: false
  needs:
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.cassandra }}-operator
{{- if ne .Values.k8sRuntime "k3d" }}
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.openebs }}
{{- end }}
  values:
  - fullnameOverride: {{ .Values.releases.cassandra }}
    scyllaImage:
      tag: {{ .Values.versions.images.scyllaServer }}
    agentImage:
      tag: {{ .Values.versions.images.scyllaAgent }}
    alternator:
      enabled: true
    developerMode: true
    serviceMonitor:
      create: false

- name: {{ .Values.releases.redis }}
  namespace: {{ .Values.namespaces.storage }}
  chart: enapter/keydb
  installed: {{ .Values.enables.storage }}
{{- if ne .Values.k8sRuntime "k3d" }}
  needs:
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.openebs }}
{{- end }}
  values:
  - fullnameOverride: {{ .Values.releases.redis | quote }}
    #imageTag: alpine_x86_64_v6.3.2
    imageTag: x86_64_v6.3.2
    nodes: {{ .Values.redis.replicas.redis }}
    password: {{ .Values.redis.password }}
    port: {{ .Values.redis.ports.readwrite }}
    configExtraArgs:
    - databases: {{ .Values.redis.databaseCount }}
    #- sentinel:
    #loadBalancer:
    #  enabled: true
    exporter:
      enabled: true
      imageTag: alpine
      extraArgs:  # ??
      - ping-on-connect: true
      - redis-only-metrics: true
    #serviceMonitor:
    #  enabled: true

- name: {{ .Values.releases.kafka }}
  namespace: {{ .Values.namespaces.storage }}
  installed: {{ .Values.enables.storage }}
  chart: "{{ requiredEnv "MYDIR" }}/charts/zer0def/redpanda"
  values:
  - fullnameOverride: {{ .Values.releases.kafka }}
    image:
      tag: "v{{ .Values.versions.images.redPanda }}"
    statefulset:
      replicas: {{ .Values.kafka.replicas }}
    config:  # https://vectorized.io/docs/configuration/ # https://docs.redpanda.com/docs/manage/cluster-maintenance/configuration/
      redpanda:
        auto_create_topics_enabled: true
        cloud_storage_access_key: {{ $s3.accessKey | quote }}
        cloud_storage_api_endpoint: {{ printf "%s-s3.%s.svc" .Values.releases.seaweedfs .Values.namespaces.storage }}
        cloud_storage_api_endpoint_port: {{ .Values.seaweedfs.ports.http.s3 }}
        cloud_storage_bucket: {{ .Values.kafka.bucket }}
        cloud_storage_disable_tls: true
        cloud_storage_enabled: true
        cloud_storage_region: us-east-1
        cloud_storage_secret_key: {{ $s3.secretKey | quote }}
        #default_topic_replications: 3
        admin:
          address: "0.0.0.0"
        rpc_server:
          address: "0.0.0.0"

{{/*
- name: swift
  namespace: {{ .Values.namespaces.storage }}
  #installed: {{ .Values.enables.storage }}
  installed: false
  chart: "{{ .Values.chartBases.swift }}/swift"
{{- if ne .Values.k8sRuntime "k3d" }}
  needs:
  - {{ .Values.namespaces.storage }}/{{ .Values.releases.openebs }}
{{- end }}
  values:
  - debug: true
    alerts:
      enabled: false
    image:
      repository: zer0def/docker-swift
      tag: latest
    haproxy:
      image:
        repository: haproxy
        tag: lts-alpine
*/}}
