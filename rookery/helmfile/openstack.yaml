bases:
- "{{ requiredEnv "MYDIR" }}/helmfile/envs.yaml"
---
helmDefaults:
  wait: true
  timeout: {{ default "86400" .Values.helmTimeout }}
  tillerless: false
  #skipDeps: true
  #createNamespace: true
  #cleanupOnFail: true

{{- $osTag := printf "%s-%s" .Values.openstack.version .Values.openstack.baseImage }}
templates:
  openstack:
    hooks:
    - &helmToolkitDependencyFixup
      events: ["prepare"]
      command: "/bin/sh"
      args:
      - "-xc"
      #- "sed -i 's#^\\(\\s*repository:\\).*#\\1 file://../../openstack-helm-infra/helm-toolkit#g' $(find {{ requiredEnv "MYDIR" }}/../charts/openstack-helm -type f -name requirements.yaml)"
      - "find {{ requiredEnv "MYDIR" }}/../charts/openstack-helm -type f -name requirements.lock -delete; sed -i 's#^\\(\\s*repository:\\).*#\\1 file://../../openstack-helm-infra/helm-toolkit#g' $(find {{ requiredEnv "MYDIR" }}/../charts/openstack-helm -type f -name requirements.yaml)"
    values:
    - &openstackCommon
{{- if .Values.openstack.tls.enabled }}
      manifests:
        certificates: true
{{- end }}
      # related: https://docs.openstack.org/nova/latest/reference/threading.html#mysql-access-and-eventlet
      conf:  # greenlet workaround
{{- range $asyncSvc := list "keystone" "glance" "cinder" "heat" "neutron" "nova" "ceilometer" "aodh" "barbican" "octavia" "designate" "magnum" "senlin" }}
        {{ $asyncSvc }}:
          oslo_messaging_rabbit:
            heartbeat_timeout_threshold: 0
            rabbit_ha_queues: true
  {{- if $.Values.openstack.tls.enabled }}
            ssl: true
            ssl_ca_file: /etc/rabbitmq/certs/ca.crt
            ssl_cert_file: /etc/rabbitmq/certs/tls.crt
            ssl_key_file: /etc/rabbitmq/certs/tls.key
  {{- end }}
{{- end }}
        logging:
          logger_root:
            level: TRACE
            handlers: stdout
{{- range $logger := list "amqp" "amqplib" "eventletwsgi" "sqlalchemy" "boto" "keystone" "glance" "cinder" "heat" "neutron" "neutron_taas" "nova" "os.brick" "placement" "ceilometer" "aodh" "barbican" "octavia" "designate" "magnum" "senlin" "manila" "trove" }}
          logger_{{ $logger }}:
            level: TRACE
{{- end }}
      endpoints:
{{/*
{{- if $.Values.openstack.tls.enabled }}
        identity:
          auth:
  {{- range (list "admin" "cinder" "glance" "heat" "heat_trustee" "heat_stack_user" "neutron" "nova" "placement" "ceilometer" "aodh" "barbican" "octavia" "designate" "magnum" "senlin" "test" }}
            {{ . }}:
              cacert: /etc/ssl/certs/openstack-helm.crt
  {{- end }}
          hosts:
            internal: keystone
          scheme:
            default: https
          port:
            api:
              default: 443
              internal: 443
  {{- range $service, $endpoint := dict "image" "glance" "image_registry" "glance-registry" "volume" "cinder" "volumev2" "cinder" "volumev3" "cinder" "orchestration" "heat" "cloudformation" "cloudformation" "cloudwatch" "cloudwatch" "network" "neutron" "load_balancer" "octavia" "dns" "designate" "baremetal" "ironic" "compute" "nova" "compute_metadata" "metadata" "compute_novnc_proxy" "novncproxy" "compute_spice_proxy" "placement" "placement" "placement" "key_manager" "barbican" "dashboard" "horizon" "metering" "ceilometer" "metric" "gnocchi" "alarming" "aodh" }}
        {{ $service }}:
          hosts:
            default: {{ $endpoint }}
          scheme:
            default: https
          port:
            api:
              default: 443
              public: 443
  {{- end }}
{{- end }}
*/}}
        oslo_db:
          auth:
            admin:
              username: {{ $.Values.credentials.database.root.user | quote }}
              password: {{ $.Values.credentials.database.root.pass | quote }}
              secret:
                tls:
{{- if $.Values.openstack.tls.enabled }}
                  internal: {{ $.Values.openstack.tls.secrets.database }}
{{- else }}
                  internal: ""
{{- end }}
{{- range $osloDbSvc := list "keystone" "glance" "cinder" "heat" "nova" "placement" "neutron" "gnocchi" "ceilometer" "aodh" "barbican" "octavia" "powerdns" "designate" "magnum" "senlin" }}
            {{ $osloDbSvc }}:
              username: {{ index (default (dict) (index $.Values.credentials.database $osloDbSvc)) "user" | quote }}
              password: {{ index (default (dict) (index $.Values.credentials.database $osloDbSvc)) "pass" | quote }}
{{- end }}
            horizon:
              username: {{ $.Values.credentials.database.horizon.user | quote }}
              password: {{ $.Values.credentials.database.horizon.pass | quote }}
              engine: "django.db.backends.postgresql"
          hosts:
            default: {{ $.Values.credentials.database.root.host | quote }}
          scheme: {{ $.Values.credentials.database.root.scheme | quote }}
          port:
            mysql:
              default: {{ $.Values.credentials.database.root.port }}
        oslo_messaging:
          hosts:
            #default: "{{ $.Values.credentials.rabbitmq.root.host }}-headless"
            default: {{ $.Values.credentials.rabbitmq.root.host }}
          statefulset: null
            #replicas: {{ $.Values.openstack.rmq.replicas }}
            #name: rabbitmq
{{- if .Values.openstack.tls.enabled }}
          port:
            https:
              default: 15672
{{- end }}
          auth:
            admin:
              username: {{ $.Values.credentials.rabbitmq.root.user | quote }}
              password: {{ $.Values.credentials.rabbitmq.root.pass | quote }}
{{- if .Values.openstack.tls.enabled }}
              secret:
                tls:
                  internal: rabbitmq-tls-direct
{{- end }}
{{- range $rabbitSvc := list "keystone" "glance" "cinder" "heat" "nova" "neutron" "ceilometer" "aodh" "barbican" "octavia" "designate" "magnum" "senlin" }}
            {{ $rabbitSvc }}:
              username: {{ index (default (dict) (index $.Values.credentials.rabbitmq $rabbitSvc)) "user" | quote }}
              password: {{ index (default (dict) (index $.Values.credentials.rabbitmq $rabbitSvc)) "pass" | quote }}
{{- end }}
        oslo_cache:
          hosts:
            default: {{ .Values.memcachedCluster }}
      images:
        pull_policy: "Always"
        tags:
          bootstrap: {{ .Values.registry }}/heat:{{ $osTag }}
          db_drop: {{ .Values.registry }}/heat:{{ $osTag }}
          db_init: {{ .Values.registry }}/heat:{{ $osTag }}
          ks_user: {{ .Values.registry }}/heat:{{ $osTag }}
          ks_endpoints: {{ .Values.registry }}/heat:{{ $osTag }}
          ks_service: {{ .Values.registry }}/heat:{{ $osTag }}
          nginx: docker.io/nginx:1.18
      network:
{{- range  list "api" "cfn" "cloudwatch" "dashboard" "metadata" "novncproxy" "osapi" "placement" "registry" "server"}}
        {{ . }}:
          ingress:
            annotations:
              nginx.org/client-max-body-size: "0"
            classes:
              namespace: {{ $.Values.ingress.namespace }}
              cluster: {{ $.Values.ingress.class }}
{{- end }}
    - labels:
        nodeSelector: &openstackCommonNodeSelector
          {{ $.Values.openstack.nodeSelectors.common | toYaml | nindent 10 }}
        computeNodeSelector: &openstackComputeNodeSelector
          <<: *openstackCommonNodeSelector
        controlNodeSelector: &openstackControlNodeSelector
          <<: *openstackCommonNodeSelector
        neutronLbNodeSelector: &openstackNeutronLbNodeSelector
          <<: *openstackCommonNodeSelector
        neutronOvsNodeSelector: &openstackNeutronOvsNodeSelector
          <<: *openstackCommonNodeSelector
        neutronSriovNodeSelector: &openstackNeutronSriovNodeSelector
          <<: *openstackCommonNodeSelector
        baremetalNodeSelector: &openstackBaremetalNodeSelector
          <<: *openstackCommonNodeSelector
      issuers:
        openstack: &openstackCAIssuer
          issuerRef:
            {{ .Values.openstack.tls.certIssuers.openstack | toYaml | nindent 12 }}

releases:
- name: keystone
  namespace: {{ .Values.openstack.namespace }}
  chart: "{{ requiredEnv "MYDIR" }}/../charts/openstack-helm/keystone"
  labels:
    app: openstack
  needs:
  - {{ .Values.certManager.namespace }}/cert-manager
  - {{ .Values.ingress.namespace }}/nginx
  - {{ .Values.openstack.namespace }}/memcached
  - {{ .Values.openstack.namespace }}/rabbitmq
  - {{ .Values.openstack.namespace }}/patroni
  - {{ .Values.openstack.namespace }}/mariadb
  - {{ .Values.openstack.namespace }}/mysql
  hooks:
  - *helmToolkitDependencyFixup
  - events: ["postsync"]
    command: "/bin/sh"
    args:
    - -xc
    - |
      OS_CLIENT_POD=os-client
      kubectl -n {{ .Values.openstack.namespace }} run "${OS_CLIENT_POD}" --image={{ .Values.registry }}/heat:{{ $osTag }} -- sleep infinity; until kubectl -n {{ .Values.openstack.namespace }} wait --for condition=ready pod "${OS_CLIENT_POD}"; do sleep 3; done &>/dev/null
      kubectl -n {{ .Values.openstack.namespace }} get secret/keystone-keystone-admin -o json | jq -r '.data|to_entries[]|"export \(.key)=\(.value|@base64d)"' | kubectl -n {{ .Values.openstack.namespace }} exec -i "${OS_CLIENT_POD}" -- tee /root/.openrc &>/dev/null
      MYCIDR="$(lxc network get "kd-ext-$(cat "${HOME}/.local/share/kubedee/clusters/rookery/network_id")" ipv4.address)"
      kubectl -n {{ .Values.openstack.namespace }} exec -i "${OS_CLIENT_POD}" -- tee /root/os-setup.sh <<EOF >/dev/null
      #!/bin/bash
      . /root/.openrc
      openstack --insecure network create --project "admin" --share --description "Provider Network - shared and external" --external --provider-network-type "flat" --provider-physical-network "{{ .Values.openstack.neutron.extBridge.network }}" provider
      openstack --insecure subnet create --project "admin" --network provider --subnet-range "${MYCIDR%.*}.0/${MYCIDR#*/}" --allocation-pool "start=${MYCIDR%.*}.2,end=${MYCIDR%.*}.254" --dns-nameserver "1.1.1.1" --gateway "${MYCIDR%/*}" provider_subnet

      # Create public network and subnet
      openstack --insecure network create --project "admin" --share --description "Public Network - shared" public
      openstack --insecure subnet create --project "admin" --network public --subnet-range "10.133.37.0/24" --allocation-pool start=10.133.37.2,end=10.133.37.254 --dns-nameserver "1.1.1.1" --gateway "10.133.37.1" public_subnet

      # Create public-ns-router
      openstack --insecure router create --project "admin" --description "Public North-South Router" public-ns-router
      openstack --insecure router set --external-gateway provider public-ns-router
      openstack --insecure router add subnet public-ns-router public_subnet
      EOF
  values:
  - *openstackCommon
  - images:
      tags:
        keystone_api: {{ .Values.registry }}/keystone:{{ $osTag }}
        keystone_credential_cleanup: {{ .Values.registry }}/heat:{{ $osTag }}
        keystone_db_sync: {{ .Values.registry }}/keystone:{{ $osTag }}
        keystone_domain_manage: {{ .Values.registry }}/keystone:{{ $osTag }}
        keystone_credential_rotate: {{ .Values.registry }}/keystone:{{ $osTag }}
        keystone_credential_setup: {{ .Values.registry }}/keystone:{{ $osTag }}
        keystone_fernet_rotate: {{ .Values.registry }}/keystone:{{ $osTag }}
        keystone_fernet_setup: {{ .Values.registry }}/keystone:{{ $osTag }}
    labels:
      api:
        <<: *openstackControlNodeSelector
      job:
        <<: *openstackControlNodeSelector
    network:
      api:
        ingress:
          annotations:
            nginx.org/rewrites: "serviceName=keystone-api rewrite=/"
    pod:
      replicas:
        api: 1
    helm3_hook: false
{{- if .Values.openstack.tls.enabled }}
    endpoints:
      identity:
        hosts:
          admin: keystone-api
          internal: keystone-api
        scheme:
          default: http
        port:
          api:
            admin: 5000
            internal: 5000
        host_fqdn_override:
          default:
            tls:
              <<: *openstackCAIssuer
              secretName: keystone-tls
    secrets:
      tls:
        identity:
          api:
            #public: keystone-tls
            public: keystone-tls-public
            internal: keystone-tls
{{- end }}

- name: glance
  namespace: {{ .Values.openstack.namespace }}
  chart: "{{ requiredEnv "MYDIR" }}/../charts/openstack-helm/glance"
  labels:
    app: openstack
  needs:
  - {{ .Values.openstack.namespace }}/ceph-openstack-config
  - {{ .Values.openstack.namespace }}/keystone
  hooks:
  - *helmToolkitDependencyFixup
  - events: ["presync"]
    command: "/bin/sh"
    args:
    - "-xec"
    - |
      until [ -n "$(kubectl -n {{ .Values.rook.namespace }} get secret/rook-ceph-client-{{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.glance }} -o jsonpath='{.data.{{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.glance }} }' | base64 -dw0)" ]; do sleep 1; done
      kubectl -n "{{ .Values.openstack.namespace }}" get secret/{{ .Values.ceph.keyringSecrets.glance }} || kubectl -n "{{ .Values.openstack.namespace }}" create secret generic "{{ .Values.ceph.keyringSecrets.glance }}" --type kubernetes.io/rbd --from-literal key="$(kubectl -n {{ .Values.rook.namespace }} get "secret/rook-ceph-client-{{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.glance }}" -o jsonpath='{.data.{{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.glance }} }' | base64 -dw0)"
  values:
  - *openstackCommon
{{- if .Values.openstack.tls.enabled }}
  - "{{ requiredEnv "MYDIR" }}/nginx-void.yaml"
  - conf:
      glance:
        keystone_authtoken:
          cafile: /etc/glance/certs/ca.crt
        glance_store:
          https_ca_certificates_file: /etc/glance/certs/ca.crt
          swift_store_cacert: /etc/glance/certs/ca.crt
          rbd_store_pool: {{ .Values.ceph.pools.glance }}
          rbd_store_user: "{{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.glance }}"
      glance_registry:
        keystone_authtoken:
          cafile: /etc/glance/certs/ca.crt
    endpoints:
      image:
        hosts:
          default: glance-api
        scheme:
          default: http
        port:
          api:
            default: 9292
        host_fqdn_override:
          default:
            tls:
              <<: *openstackCAIssuer
              secretName: glance-api-tls
      image_registry:
        hosts:
          default: glance-registry
        scheme:
          default: http
        port:
          api:
            default: 9191
        host_fqdn_override:
          default:
            tls:
              <<: *openstackCAIssuer
              secretName: glance-registry-tls
    secrets:
      rbd: {{ .Values.ceph.keyringSecrets.glance }}
      tls:
        image:
          api:
            #public: glance-api-tls
            public: glance-api-tls-public
            internal: glance-api-tls
        image_registry:
          api:
            #public: glance-registry-tls
            public: glance-registry-tls-public
            internal: glance-registry-tls
    #network:
    #  api:
    #    ingress:
    #      annotations:
    #        nginx.org/ssl-services: "glance-api"
    #  registry:
    #    ingress:
    #      annotations:
    #        nginx.org/ssl-services: "glance-registry"
{{- end }}
  - storage: rbd
    bootstrap:
      structured:
        images:  # https://docs.openstack.org/image-guide/obtain-images.html
{{ range $release, $archs := dict "0.6.1" (list "aarch64" "arm" "ppc64le" "x86_64") }}{{ range $archs }}
          "00-cirros-{{ $release }}-{{ . }}":
            id: null
            name: "cirros-{{ $release }}-{{ . }}"
            source_url: "http://download.cirros-cloud.net/{{ $release }}/"
            image_file: "cirros-{{ $release }}-{{ . }}-disk.img"
            min_disk: 1
            image_type: qcow2
            container_format: bare
            properties:
              architecture: {{ . }}
              hw_boot_menu: "true"
              #hw_vif_model: virtio
              #hw_vif_multiqueue_enabled: "true"
              #hw_watchdog_action: none
              os_admin_user: cirros
              os_distro: cirros
              os_type: linux
              os_version: {{ $release | quote }}
{{ end }}{{ end }}
{{ range list "xena" "yoga" }}
          amphora-{{ . }}-amd64:
            id: null
            source_url: "https://minio.services.osism.tech/openstack-octavia-amphora-image/"
            name: "amphora-{{ . }}-amd64"
            image_file: "octavia-amphora-haproxy-{{ . }}.qcow2"
            min_disk: 2
            min_ram: 1024
            image_type: qcow2
            container_format: bare
            tag: {{ $.Values.openstack.octavia.amphora.image_tag | quote }}
            properties:  # https://docs.openstack.org/glance/latest/admin/useful-image-properties.html
              architecture: x86_64
              #hw_disk_bus: scsi
              #hw_scsi_model: virtio-scsi
              hw_vif_model: virtio
              hw_vif_multiqueue_enabled: "true"
              hw_watchdog_action: none
              os_admin_user: ubuntu
              os_distro: ubuntu
              os_type: linux
              os_version: "20.04"
{{ end }}
          archlinux-amd64:
            id: null
            name: "archlinux-amd64"
            source_url: "https://geo.mirror.pkgbuild.com/images/latest/"
            image_file: "Arch-Linux-x86_64-cloudimg.qcow2"
            min_disk: 8
            image_type: qcow2
            container_format: bare
            properties:  # https://docs.openstack.org/glance/latest/admin/useful-image-properties.html
              architecture: x86_64
              hw_boot_menu: "true"
              hw_vif_model: virtio
              hw_vif_multiqueue_enabled: "true"
              hw_watchdog_action: none
              os_admin_user: arch
              os_distro: arch
              os_type: linux
{{/*
{{ range $release, $archs := dict "6" (list "x86_64") }}{{ range $archs }}
          "centos-{{ $release }}-{{ . }}":  # https://cloud.centos.org/centos/
            id: null
            name: "centos-{{ $release }}-{{ . }}"
            source_url: "http://cloud.centos.org/centos/{{ $release }}/images/"
            image_file: "CentOS-{{ $release }}-{{ . }}-GenericCloud.qcow2"
            min_disk: 8
            image_type: qcow2
            container_format: bare
            properties:
              architecture: {{ . }}
              hw_boot_menu: "true"
              hw_vif_model: virtio
              hw_vif_multiqueue_enabled: "true"
              hw_watchdog_action: none
              os_admin_user: centos
              os_distro: centos
              os_type: linux
              os_version: {{ $release | quote }}
{{ end }}{{ end }}
*/}}
          cirros: null
{{ range $release, $archs := dict "9" (list "amd64" "arm64") "10" (list "amd64" "arm64") }}{{ range $archs }}
          "debian-{{ $release }}-{{ . }}":
            id: null
            name: "debian-{{ $release }}-{{ . }}"
            source_url: "http://cdimage.debian.org/cdimage/openstack/current-{{ $release }}/"
            image_file: "debian-{{ $release }}-openstack-{{ . }}.qcow2"
            min_disk: 8
            image_type: qcow2
            container_format: bare
            properties:
              #architecture: {{ . }}
              hw_boot_menu: "true"
              hw_vif_model: virtio
              hw_vif_multiqueue_enabled: "true"
              hw_watchdog_action: none
              os_admin_user: debian
              os_distro: debian
              os_type: linux
              os_version: {{ $release | quote }}
{{ end }}{{ end }}
{{/*
{{ range $release, $archs := dict "6" (list "x86_64") }}{{ range $archs }}
          "opensuse-{{ $release }}-{{ . }}":  # https://get.opensuse.org/tumbleweed/
            id: null
            name: "opensuse-{{ $release }}-{{ . }}"
            source_url: "??"
            image_file: "??"
            min_disk: 8
            image_type: qcow2
            container_format: bare
            properties:
              #architecture: ??
              hw_boot_menu: "true"
              hw_vif_model: virtio
              hw_vif_multiqueue_enabled: "true"
              hw_watchdog_action: none
              os_admin_user: opensuse
              os_distro: opensuse
              os_type: linux
              os_version: ??
{{ end }}{{ end }}
*/}}
{{ range $release, $archs := dict "14.04" (list "amd64" "arm64" "armhf" "i386" "ppc64el") "16.04" (list "amd64" "arm64" "armhf" "i386" "powerpc" "ppc64el" "s390x") }}{{ range $archs }}
          "ubuntu-{{ $release }}-{{ . }}":
            id: null
            name: "ubuntu-{{ $release }}-{{ . }}"
            source_url: "https://cloud-images.ubuntu.com/releases/{{ $release }}/release/"
            image_file: "ubuntu-{{ $release }}-server-cloudimg-{{ . }}-disk1.img"
            min_disk: 8
            image_type: qcow2
            container_format: bare
            properties:
              #architecture: {{ . }}
              hw_boot_menu: "true"
              hw_vif_model: virtio
              hw_vif_multiqueue_enabled: "true"
              hw_watchdog_action: none
              os_admin_user: ubuntu
              os_distro: ubuntu
              os_type: linux
              os_version: {{ $release | quote }}
{{ end }}{{ end }}
{{ range $release, $archs := dict "18.04" (list "amd64" "arm64" "armhf" "i386" "ppc64el" "s390x") "20.04" (list "amd64" "arm64" "armhf" "ppc64el" "s390x") "22.04" (list "amd64" "arm64" "armhf" "ppc64el" "riscv64" "s390x") }}{{ range $archs }}
          "ubuntu-{{ $release }}-{{ . }}":
            id: null
            name: "ubuntu-{{ $release }}-{{ . }}"
            source_url: "https://cloud-images.ubuntu.com/releases/{{ $release }}/release/"
            image_file: "ubuntu-{{ $release }}-server-cloudimg-{{ . }}.img"
            min_disk: 8
            image_type: qcow2
            container_format: bare
            properties:
              #architecture: {{ . }}
              hw_boot_menu: "true"
              hw_vif_model: virtio
              hw_vif_multiqueue_enabled: "true"
              hw_watchdog_action: none
              os_admin_user: ubuntu
              os_distro: ubuntu
              os_type: linux
              os_version: {{ $release | quote }}
{{ end }}{{ end }}
    ceph_client:
      user_secret_name: {{ .Values.ceph.keyringSecrets.admin }}
    conf:
      glance:
        DEFAULT:
          bind_host: 0.0.0.0
          workers: 4
        glance_store:
          rbd_store_replication: 2
          # adopt OSD failure domain CRUSH rule from the `replicapool` block pool
          #rbd_store_crush_rule: {{ .Values.rook.replicaPool }}
          rbd_store_crush_rule: replicated_rule
          rbd_thin_provisioning: true
    images:
      tags:
        glance_api: {{ .Values.registry }}/glance:{{ $osTag }}
        glance_db_sync: {{ .Values.registry }}/glance:{{ $osTag }}
        glance_metadefs_load: {{ .Values.registry }}/glance:{{ $osTag }}
        glance_registry: {{ .Values.registry }}/glance:{{ $osTag }}
        glance_storage_init: {{ .Values.registry }}/ceph-config-helper:{{ $osTag }}
    labels:
      api:
        <<: *openstackControlNodeSelector
      job:
        <<: *openstackControlNodeSelector
      registry:
        <<: *openstackControlNodeSelector
    pod:
      replicas:
        api: 1
        registry: 1
    network:
      api:
        ingress:
          annotations:
            nginx.org/rewrites: "serviceName=glance-api rewrite=/"
            nginx.org/websocket-services: glance-api
      registry:
        ingress:
          annotations:
            nginx.org/rewrites: "serviceName=glance-registry rewrite=/"
            #nginx.org/websocket-services: glance-registry
    helm3_hook: false
    manifests:
      # disable storage initialization, as we're creating it ourselves, so that erasure-coded pools work
      job_storage_init: false
      # image registry deprecation due to fold into api
      #deployment_registry: true
      #ingress_registry: true
      #pdb_registry: true
      #service_ingress_registry: true
      #service_registry: true
    dependencies:
      static:
        api:
          jobs:
          - glance-db-sync
          - glance-rabbit-init
          - glance-ks-user
          - glance-ks-endpoints

- name: cinder
  namespace: {{ .Values.openstack.namespace }}
  chart: "{{ requiredEnv "MYDIR" }}/../charts/openstack-helm/cinder"
  labels:
    app: openstack
  needs:
  - {{ .Values.openstack.namespace }}/ceph-openstack-config
  - {{ .Values.openstack.namespace }}/keystone
  hooks:
  - *helmToolkitDependencyFixup
  - events: ["presync"]
    command: "/bin/sh"
    args:
    - "-xec"
    - |
      until [ -n "$(kubectl -n {{ .Values.rook.namespace }} get secret/rook-ceph-client-{{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.cinderVolume }} -o jsonpath='{.data.{{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.cinderVolume }} }' | base64 -dw0)" ]; do sleep 1; done;
      kubectl -n "{{ .Values.openstack.namespace }}" create secret generic "{{ .Values.ceph.keyringSecrets.cinderVolume }}" --type kubernetes.io/rbd --from-literal key="$(kubectl -n {{ .Values.rook.namespace }} get "secret/rook-ceph-client-{{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.cinderVolume }}" -o jsonpath='{.data.{{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.cinderVolume }} }' | base64 -dw0)"
      until [ -n "$(kubectl -n {{ .Values.rook.namespace }} get secret/rook-ceph-client-{{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.cinderBackup }} -o jsonpath='{.data.{{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.cinderBackup }} }' | base64 -dw0)" ]; do sleep 1; done
      kubectl -n "{{ .Values.openstack.namespace }}" create secret generic "{{ .Values.ceph.keyringSecrets.cinderBackup }}" --type kubernetes.io/rbd --from-literal key="$(kubectl -n {{ .Values.rook.namespace }} get "secret/rook-ceph-client-{{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.cinderBackup }}" -o jsonpath='{.data.{{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.cinderBackup }} }' | base64 -dw0)"
  values:
  - *openstackCommon
  - storage: ceph
    conf:
      backends:
        rbd1:
          rbd_pool: {{ .Values.ceph.pools.cinderVolume | quote }}
          rbd_user: "{{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.cinderVolume }}"
{{/*
      ceph:
        pools:
          {{ .Values.ceph.pools.cinderBackup }}:
            replication: 2
            # adopt OSD failure domain CRUSH rule from the `replicapool` block pool
            #crush_rule: {{ .Values.rook.replicaPool }}
            crush_rule: replicated_rule
            chunk_size: 8
            app_name: cinder-backup
          {{ .Values.ceph.pools.cinderVolume }}:
            replication: 2
            # adopt OSD failure domain CRUSH rule from the `replicapool` block pool
            #crush_rule: {{ .Values.rook.replicaPool }}
            crush_rule: replicated_rule
            chunk_size: 8
            app_name: cinder-volume
*/}}
      cinder:
        DEFAULT:
          backup_driver: "cinder.backup.drivers.ceph.CephBackupDriver"
          backup_ceph_conf: /etc/ceph/ceph.conf
          backup_ceph_user:  {{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.cinderBackup }}
          backup_ceph_pool: {{ .Values.ceph.pools.cinderBackup }}
        coordination:
          backend_url: "postgresql://{{ $.Values.credentials.database.cinder.user }}:{{ $.Values.credentials.database.cinder.pass }}@{{ $.Values.credentials.database.root.host }}/cinder"
    images:
      tags:
        cinder_api: {{ .Values.registry }}/cinder:{{ $osTag }}
        cinder_db_sync: {{ .Values.registry }}/cinder:{{ $osTag }}
        cinder_storage_init: {{ .Values.registry }}/ceph-config-helper:{{ $osTag }}
        cinder_scheduler: {{ .Values.registry }}/cinder:{{ $osTag }}
        cinder_volume: {{ .Values.registry }}/cinder:{{ $osTag }}
        cinder_volume_usage_audit: {{ .Values.registry }}/cinder:{{ $osTag }}
        cinder_backup: {{ .Values.registry }}/cinder:{{ $osTag }}
        cinder_backup_storage_init: {{ .Values.registry }}/ceph-config-helper:{{ $osTag }}
    labels:
      api:
        <<: *openstackControlNodeSelector
      backup:
        <<: *openstackControlNodeSelector
      job:
        <<: *openstackControlNodeSelector
      scheduler:
        <<: *openstackControlNodeSelector
      volume:
        <<: *openstackControlNodeSelector
    network:
      api:
        ingress:
          annotations:
            nginx.org/rewrites: "serviceName=cinder-api rewrite=/"
    pod:
      replicas:
        api: 1
        volume: 1
        scheduler: 1
        backup: 1
      security_context:
        cinder_volume:
          container:
            cinder_volume:
              capabilities:
                add:
                - CAP_AUDIT_WRITE
    manifests:
      # disable storage initialization, as we're creating it ourselves, so that erasure-coded pools work
      job_backup_storage_init: false
      job_storage_init: false
      network_policy: true
    dependencies:
      static:
{{- range (list "api" "backup" "scheduler" "volume" "volume_usage_audit") }}
        {{ . }}:
          jobs:
          - cinder-db-sync
          - cinder-ks-user
          - cinder-ks-endpoints
          - cinder-rabbit-init
{{- end }}
    ceph_client:
      user_secret_name: {{ .Values.ceph.keyringSecrets.admin }}
    helm3_hook: false
    secrets:
      rbd:
        backup: {{ .Values.ceph.keyringSecrets.cinderBackup }}
        volume: {{ .Values.ceph.keyringSecrets.cinderVolume }}
        volume_external: {{ .Values.ceph.pools.cinderVolumeExt }}
{{- if .Values.openstack.tls.enabled }}
  - conf:
      cinder:
        DEFAULT:
          glance_ca_certificates_file: /etc/cinder/certs/ca.crt
        keystone_authtoken:
          cafile: /etc/cinder/certs/ca.crt
      #software:
      #  apache2:
      #    binary: cinder-api
      #    start_parameters: "--config-file /etc/cinder/cinder.conf"
      #    a2enmod: null
      #    a2dismod: null
    secrets:
      tls:
        #volumev3:
        volume:
          api:
            #public: cinder-v3-tls
            public: cinder-v3-tls-public
            internal: cinder-v3-tls
    endpoints:
      #volumev3:
      volume:
        hosts:
          default: cinder-api
        scheme:
          default: http
        port:
          api:
            default: 8776
        host_fqdn_override:
          default:
            tls:
              <<: *openstackCAIssuer
              secretName: cinder-v3-tls
              dnsNames:
              - "cinder"
              - "cinder.{{ .Values.openstack.namespace }}"
              - "cinder.{{ .Values.openstack.namespace }}.svc"
              - "cinder.{{ .Values.openstack.namespace }}.svc.cluster.local"
              - "cinder-api"
              - "cinder-api.{{ .Values.openstack.namespace }}"
              - "cinder-api.{{ .Values.openstack.namespace }}.svc"
              - "cinder-api.{{ .Values.openstack.namespace }}.svc.cluster.local"
{{- end }}

- name: heat
  namespace: {{ .Values.openstack.namespace }}
  chart: "{{ requiredEnv "MYDIR" }}/../charts/openstack-helm/heat"
  labels:
    app: openstack
  needs:
  - {{ .Values.openstack.namespace }}/keystone
  hooks:
  - *helmToolkitDependencyFixup
  values:
  - *openstackCommon
{{- if .Values.openstack.tls.enabled }}
  - conf:
      heat:
        clients_neutron:
          ca_file: /etc/heat/certs/ca.crt
        clients_cinder:
          ca_file: /etc/heat/certs/ca.crt
        clients_glance:
          ca_file: /etc/heat/certs/ca.crt
        clients_nova:
          ca_file: /etc/heat/certs/ca.crt
        clients_swift:
          ca_file: /etc/heat/certs/ca.crt
        ssl:
          ca_file: /etc/heat/certs/ca.crt
        keystone_authtoken:
          cafile: /etc/heat/certs/ca.crt
        clients:
          ca_file: /etc/heat/certs/ca.crt
        clients_keystone:
          ca_file: /etc/heat/certs/ca.crt
    endpoints:
      orchestration:
        hosts:
          default: heat-api
        scheme:
          default: http
        port:
          api:
            default: 8004
        host_fqdn_override:
          default:
            tls:
              <<: *openstackCAIssuer
              secretName: heat-api-tls
      cloudformation:
        hosts:
          default: heat-cfn
        scheme:
          default: http
        port:
          api:
            default: 8000
        host_fqdn_override:
          default:
            tls:
              <<: *openstackCAIssuer
              secretName: heat-cfn-tls
      cloudwatch:
        hosts:
          default: heat-cloudwatch
        scheme:
          default: http
        port:
          api:
            default: 8003
        host_fqdn_override:
          default:
            tls:
              <<: *openstackCAIssuer
              secretName: heat-cloudwatch-tls
    secrets:
      tls:
        orchestration:
          api:
            #public: heat-api-tls
            public: heat-api-tls-public
            internal: heat-api-tls
        cloudformation:
          cfn:
            #public: heat-cfn-tls
            public: heat-cfn-tls-public
            internal: heat-cfn-tls
{{- end }}
  - images:
      tags:
        heat_api: {{ .Values.registry }}/heat:{{ $osTag }}
        heat_cfn: {{ .Values.registry }}/heat:{{ $osTag }}
        heat_cloudwatch: {{ .Values.registry }}/heat:{{ $osTag }}
        heat_db_sync: {{ .Values.registry }}/heat:{{ $osTag }}
        heat_engine: {{ .Values.registry }}/heat:{{ $osTag }}
        heat_engine_cleaner: {{ .Values.registry }}/heat:{{ $osTag }}
        heat_purge_deleted: {{ .Values.registry }}/heat:{{ $osTag }}
    labels:
      api:
        <<: *openstackControlNodeSelector
      cfn:
        <<: *openstackControlNodeSelector
      cloudwatch:
        <<: *openstackControlNodeSelector
      engine:
        <<: *openstackControlNodeSelector
      job:
        <<: *openstackControlNodeSelector
    pod:
      replicas:
        api: 1
        cfn: 1
        cloudwatch: 1
        engine: 1
    helm3_hook: false
    network:
      api:
        ingress:
          annotations:
            nginx.org/rewrites: "serviceName=heat-api rewrite=/"
      cfn:
        ingress:
          annotations:
            nginx.org/rewrites: "serviceName=heat-cfn rewrite=/"
      #cloudwatch:
      #  ingress:
      #    annotations:
      #      nginx.org/rewrites: "serviceName=heat-api rewrite=/"

- name: neutron
  namespace: {{ .Values.openstack.namespace }}
  chart: "{{ requiredEnv "MYDIR" }}/../charts/openstack-helm/neutron"
  labels:
    app: openstack
  needs:
  - {{ .Values.openstack.namespace }}/openvswitch
  - {{ .Values.openstack.namespace }}/keystone
  hooks:
  - *helmToolkitDependencyFixup
  values:
  - *openstackCommon
{{- if .Values.openstack.tls.enabled }}
  - "{{ requiredEnv "MYDIR" }}/../charts/openstack-helm/neutron/values_overrides/tls.yaml"  # testing
  #- "{{ requiredEnv "MYDIR" }}/nginx-void.yaml"
  - conf:
      neutron:
        DEFAULT:
          bind_host: 127.0.0.1
        #  bind_host: 0.0.0.0
        nova:
          cafile: /etc/neutron/certs/ca.crt
        keystone_authtoken:
          cafile: /etc/neutron/certs/ca.crt
      metadata_agent:
        DEFAULT:
          auth_ca_cert: /etc/ssl/certs/openstack-helm.crt
          nova_metadata_port: 443
          nova_metadata_protocol: https
    endpoints:
      identity:
        scheme: http
      network:
        hosts:
          default: neutron-server
        port:
          api:
            default: 9696
        host_fqdn_override:
          default:
            tls:
              <<: *openstackCAIssuer
              secretName: neutron-tls
      oslo_messaging:
        port:
          https:
            default: 15672
    #network:
    #  server:
    #    ingress:
    #      annotations:
    #        nginx.org/ssl-services: "neutron-server"
    secrets:
      tls:
        compute_metadata:
          metadata:
            internal: nova-metadata-tls
        network:
          server:
            #public: neutron-tls
            public: neutron-tls-public
            internal: neutron-tls
{{- end }}
  - images:
      tags:
        neutron_db_sync: {{ .Values.registry }}/neutron:{{ $osTag }}
        neutron_server: {{ .Values.registry }}/neutron:{{ $osTag }}
        neutron_dhcp: {{ .Values.registry }}/neutron:{{ $osTag }}
        neutron_metadata: {{ .Values.registry }}/neutron:{{ $osTag }}
        neutron_l3: {{ .Values.registry }}/neutron:{{ $osTag }}
        neutron_l2gw: {{ .Values.registry }}/neutron:{{ $osTag }}
        neutron_openvswitch_agent: {{ .Values.registry }}/neutron:{{ $osTag }}
        neutron_linuxbridge_agent: {{ .Values.registry }}/neutron:{{ $osTag }}
        neutron_bagpipe_bgp: {{ .Values.registry }}/neutron:{{ $osTag }}
        neutron_ironic_agent: {{ .Values.registry }}/neutron:{{ $osTag }}
        neutron_netns_cleanup_cron: {{ .Values.registry }}/neutron:{{ $osTag }}
    labels:
      agent:
        dhcp:
          <<: *openstackControlNodeSelector
        l3:
          <<: *openstackControlNodeSelector
        metadata:
          <<: *openstackControlNodeSelector
        l2gw:
          <<: *openstackControlNodeSelector
      job:
        <<: *openstackControlNodeSelector
      server:
        <<: *openstackControlNodeSelector
      ironic_agent:
        <<: *openstackControlNodeSelector
      netns_cleanup_cron:
        <<: *openstackControlNodeSelector
      lb:  # linuxbridge
        <<: *openstackNeutronLbNodeSelector
      ovs:  # openvswitch
        <<: *openstackNeutronOvsNodeSelector
      #sriov:  # sriov
      #  <<: *openstackNeutronSriovNodeSelector
      bagpipe_bgp:
        <<: *openstackComputeNodeSelector
    network:
      #interface:
      #  tunnel: {{ $.Values.openstack.neutron.extBridge.iface }}
      server:
        ingress:
          annotations:
            nginx.org/rewrites: "serviceName=neutron-server rewrite=/"
            nginx.org/websocket-services: neutron-server
    conf:
      neutron:
        DEFAULT:
          l3_ha: False
          max_l3_agents_per_router: 2
          l3_ha_network_type: vxlan
          dhcp_agents_per_network: 2
      plugins:
        ml2_conf:
          ml2_type_flat:
            flat_networks: {{ $.Values.openstack.neutron.extBridge.network }}
        openvswitch_agent:
          agent:
            tunnel_types: vxlan
          ovs:
            bridge_mappings: "{{ $.Values.openstack.neutron.extBridge.network }}:{{ $.Values.openstack.neutron.extBridge.name }}"
      rabbitmq:
        policies:
        - vhost: "neutron"
          name: "ha_ttl_neutron"
          definition:
            ha-mode: "all"
            ha-sync-mode: "automatic"
            message-ttl: 120000
          priority: 0
          apply-to: all
          # only pattern changed
          pattern: '^(?!amq\.).*'
      auto_bridge_add:
        {{ .Values.openstack.neutron.extBridge.name }}: {{ .Values.openstack.neutron.extBridge.iface }}
    resources:
      enabled: true
    pod:
      probes:
        rpc_timeout: 300
      replicas:
        server: 1
        ironic_agent: 1
      use_fqdn:
        neutron_agent: false
    manifests:
      daemonset_sriov_agent: false
    helm3_hook: false  # circular dependency server → job

{{- $cephKeyring := printf "%s-%s" .Values.openstack.namespace .Values.ceph.keyringSecrets.cinderVolume }}
- name: nova
  namespace: {{ .Values.openstack.namespace }}
  chart: "{{ requiredEnv "MYDIR" }}/../charts/openstack-helm/nova"
  labels:
    app: openstack
  needs:
  - {{ .Values.openstack.namespace }}/ceph-openstack-config
  - {{ .Values.openstack.namespace }}/keystone
  #- {{ .Values.openstack.namespace }}/libvirt
  hooks:
  - *helmToolkitDependencyFixup
  values:
  - *openstackCommon
{{- if .Values.openstack.tls.enabled }}
  #- "{{ requiredEnv "MYDIR" }}/../charts/openstack-helm/nova/values_overrides/tls.yaml"
  - conf:
      nova:
        glance:
          cafile: /etc/nova/certs/ca.crt
        ironic:
          cafile: /etc/nova/certs/ca.crt
        neutron:
          cafile: /etc/nova/certs/ca.crt
        keystone_authtoken:
          cafile: /etc/nova/certs/ca.crt
        cinder:
          cafile: /etc/nova/certs/ca.crt
        placement:
          cafile: /etc/nova/certs/ca.crt
        keystone:
          cafile: /etc/nova/certs/ca.crt
    endpoints:
      compute:
        hosts:
          default: nova-api
        scheme:
          default: http
        port:
          api:
            default: 8774
          novncproxy:
            default: 6080
        host_fqdn_override:
          default:
            tls:
              <<: *openstackCAIssuer
              secretName: nova-api-tls
      compute_metadata:
        hosts:
          default: nova-metadata
        scheme:
          default: http
        port:
          metadata:
            default: 8775
        host_fqdn_override:
          default:
            tls:
              <<: *openstackCAIssuer
              secretName: nova-metadata-tls
              dnsNames:
              - "metadata"
              - "metadata.{{ .Values.openstack.namespace }}"
              - "metadata.{{ .Values.openstack.namespace }}.svc"
              - "metadata.{{ .Values.openstack.namespace }}.svc.cluster.local"
              - "nova-metadata"
              - "nova-metadata.{{ .Values.openstack.namespace }}"
              - "nova-metadata.{{ .Values.openstack.namespace }}.svc"
              - "nova-metadata.{{ .Values.openstack.namespace }}.svc.cluster.local"
      compute_novnc_proxy:
        hosts:
          default: nova-novncproxy
        scheme:
          default: http
        port:
          novnc_proxy:
            default: 6080
        host_fqdn_override:
          default:
            tls:
              <<: *openstackCAIssuer
              secretName: nova-novncproxy-tls
      compute_serial_proxy:
        hosts:
          default: nova-serialproxy
        scheme:
          default: http
        port:
          serial_proxy:
            default: 6084
        host_fqdn_override:
          default:
            tls:
              <<: *openstackCAIssuer
              secretName: nova-serialproxy-tls
      compute_spice_proxy:  # just to pad out certificate creation?
        hosts:
          default: nova-spiceproxy
        scheme:
          default: http
        port:
          spice_proxy:
            default: 6082
        host_fqdn_override:
          default:
            tls:
              <<: *openstackCAIssuer
              secretName: nova-spiceproxy-tls
      #placement:
      #  hosts:
      #    default: placement-api
      #  scheme:
      #    default: http
      #  port:
      #    spice_proxy:
      #      default: 8778
      #  host_fqdn_override:
      #    default:
      #      tls:
      #        <<: *openstackCAIssuer
      #        secretName: nova-placement-tls
    secrets:
      tls:
        compute:
          osapi:
            #public: nova-api-tls
            public: nova-api-tls-public
            internal: nova-api-tls
        compute_metadata:
          metadata:
            #public: nova-metadata-tls
            public: nova-metadata-tls-public
            internal: nova-metadata-tls
        compute_novnc_proxy:
          novncproxy:
            #public: nova-novncproxy-tls
            public: nova-novncproxy-tls-public
            internal: nova-novncproxy-tls
        compute_serial_proxy:
          serialproxy:
            #public: nova-serialproxy-tls
            public: nova-serialproxy-tls-public
            internal: nova-serialproxy-tls
        compute_spice_proxy:
          spiceproxy:
            #public: nova-spiceproxy-tls
            public: nova-spiceproxy-tls-public
            internal: nova-spiceproxy-tls
        #placement:
        #  placement:
        #    #public: nova-placement-tls
        #    public: nova-placement-tls-public
        #    internal: nova-placement-tls
    #network:
    #  osapi:
    #    ingress:
    #      annotations:
    #        nginx.org/ssl-services: "nova-api"
    #  metadata:
    #    ingress:
    #      annotations:
    #        nginx.org/ssl-services: "nova-metadata"
    #  novncproxy:
    #    ingress:
    #      annotations:
    #        nginx.org/ssl-services: "nova-novncproxy"
    #  placement:
    #    ingress:
    #      annotations:
    #        nginx.org/ssl-services: "placement-api"
{{- end }}
  - images:
      tags:
        nova_api: {{ .Values.registry }}/nova:{{ $osTag }}
        nova_cell_setup: {{ .Values.registry }}/nova:{{ $osTag }}
        nova_cell_setup_init: {{ .Values.registry }}/heat:{{ $osTag }}
        nova_compute: {{ .Values.registry }}/nova:{{ $osTag }}
        nova_compute_ironic: docker.io/kolla/ubuntu-source-nova-compute-ironic:{{ .Values.openstack.version }}
        nova_compute_ssh: {{ .Values.registry }}/nova:{{ $osTag }}
        nova_conductor: {{ .Values.registry }}/nova:{{ $osTag }}
        nova_consoleauth: {{ .Values.registry }}/nova:{{ $osTag }}
        nova_db_sync: {{ .Values.registry }}/nova:{{ $osTag }}
        nova_novncproxy: {{ .Values.registry }}/nova:{{ $osTag }}
        #nova_novncproxy_assets: docker.io/kolla/ubuntu-source-nova-novncproxy:{{ .Values.openstack.version }}
        nova_novncproxy_assets: docker.io/kolla/ubuntu-source-nova-novncproxy:wallaby
        nova_placement: {{ .Values.registry }}/nova:{{ $osTag }}
        nova_scheduler: {{ .Values.registry }}/nova:{{ $osTag }}
        nova_serialproxy: {{ .Values.registry }}/nova:{{ $osTag }}
        nova_serialproxy_assets: {{ .Values.registry }}/nova:{{ $osTag }}
        nova_service_cleaner: {{ .Values.registry }}/ceph-config-helper:{{ $osTag }}
        nova_spiceproxy: {{ .Values.registry }}/nova:{{ $osTag }}
        nova_spiceproxy_assets: {{ .Values.registry }}/nova:{{ $osTag }}
    labels:
      agent:
        compute:
          <<: *openstackComputeNodeSelector
        compute_ironic:
          <<: *openstackComputeNodeSelector
      api_metadata:
        <<: *openstackControlNodeSelector
      conductor:
        <<: *openstackControlNodeSelector
      consoleauth:
        <<: *openstackControlNodeSelector
      job:
        <<: *openstackControlNodeSelector
      novncproxy:
        <<: *openstackControlNodeSelector
      osapi:
        <<: *openstackControlNodeSelector
      placement:
        <<: *openstackControlNodeSelector
      scheduler:
        <<: *openstackControlNodeSelector
      serialproxy:
        <<: *openstackControlNodeSelector
      spiceproxy:
        <<: *openstackControlNodeSelector
    helm3_hook: false  # circular dependency api ↔ job
    network:
      osapi:
        ingress:
          annotations:
            nginx.org/rewrites: "serviceName=nova-api rewrite=/"
      metadata:
        ingress:
          annotations:
            nginx.org/rewrites: "serviceName=nova-metadata rewrite=/"
      novncproxy:
        ingress:
          annotations:
            nginx.org/rewrites: "serviceName=nova-novncproxy rewrite=/"
            nginx.org/websocket-services: nova-novncproxy
            ingress.kubernetes.io/ssl-redirect: "False"  # insecure, I know
      serialproxy:
        node_port:
          enabled: false
          port: 30684
        ingress:
          annotations:
            nginx.org/rewrites: "serviceName=nova-serialproxy rewrite=/"
            nginx.org/websocket-services: nova-serialproxy
            ingress.kubernetes.io/ssl-redirect: "False"  # insecure, I know
      spiceproxy:
        ingress:
          annotations:
            nginx.org/rewrites: "serviceName=nova-spiceproxy rewrite=/"
            nginx.org/websocket-services: nova-spiceproxy
            ingress.kubernetes.io/ssl-redirect: "False"  # insecure, I know
      #placement:
      #  ingress:
      #    annotations:
      #      nginx.org/rewrites: "serviceName=placement-api rewrite=/"
    endpoints:
      oslo_db_api:
        auth:
          admin:
            username: {{ $.Values.credentials.database.root.user | quote }}
            password: {{ $.Values.credentials.database.root.pass | quote }}
          nova:
            username: {{ $.Values.credentials.database.nova.user | quote }}
            password: {{ $.Values.credentials.database.nova.pass | quote }}
        hosts:
          default: {{ $.Values.credentials.database.root.host | quote }}
        scheme: {{ $.Values.credentials.database.root.scheme | quote }}
        port:
          mysql:
            default: {{ $.Values.credentials.database.root.port }}
      oslo_db_cell0:
        auth:
          admin:
            username: {{ $.Values.credentials.database.root.user | quote }}
            password: {{ $.Values.credentials.database.root.pass | quote }}
          nova:
            username: {{ $.Values.credentials.database.nova.user | quote }}
            password: {{ $.Values.credentials.database.nova.pass | quote }}
        hosts:
          default: {{ $.Values.credentials.database.root.host | quote }}
        scheme: {{ $.Values.credentials.database.root.scheme | quote }}
        port:
          mysql:
            default: {{ $.Values.credentials.database.root.port }}
    manifests:
      deployment_consoleauth: false
      deployment_placement: false
      ingress_placement: false
      job_db_init_placement: false
      job_ks_placement_endpoints: false
      job_ks_placement_service: false
      job_ks_placement_user: false
      pdb_placement: false
      secret_keystone_placement: false
      service_ingress_placement: false
      service_placement: false
    resources:
      enabled: true
    pod:
      probes:
        rpc_timeout: 300
      replicas:
        api_metadata: 1
        osapi: 1
        conductor: 1
        scheduler: 1
        novncproxy: 1
        serialproxy: 1
        #spiceproxy: 1
      use_fqdn:
        compute: false
    bootstrap:
      structured:
        flavors:
          options:
            m1_tiny:
              id: "m1_tiny"
              name: "m1.tiny"
              ram: 512
              disk: 8
              vcpus: 1
            m1_small:
              id: "m1_small"
              name: "m1.small"
              ram: 2048
              disk: 20
              vcpus: 1
            m1_medium:
              id: "m1_medium"
              name: "m1.medium"
              ram: 4096
              disk: 40
              vcpus: 2
            m1_large:
              id: "m1_large"
              name: "m1.large"
              ram: 8192
              disk: 80
              vcpus: 4
            m1_xlarge:
              id: "m1_xlarge"
              name: "m1.xlarge"
              ram: 16384
              disk: 160
              vcpus: 8
    conf:
      enable_iscsi: true
      ceph:
        cinder:
          secret_uuid: {{ .Values.ceph.libvirtUUID }}
          user: {{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.cinderVolume }}
          keyring: {{ exec "/bin/sh" (list "-ec" (printf "until kubectl -n %s get secret/%s-client-%s &>/dev/null; do sleep 1; done; kubectl -n %s get secret/%s-client-%s -o jsonpath='{.data.%s}' | base64 -dw0" .Values.rook.namespace .Values.rook.namespace $cephKeyring .Values.rook.namespace .Values.rook.namespace $cephKeyring $cephKeyring)) | quote }}
          ecpool: "{{ .Values.ceph.pools.nova }}{{ .Values.ceph.osEcPoolSuffix }}"
      rabbitmq:
        policies:
        - vhost: "nova"
          name: "ha_ttl_nova"
          definition:
            ha-mode: "all"
            ha-sync-mode: "automatic"
            message-ttl: 120000
          priority: 0
          apply-to: all
          # only pattern changed
          pattern: '^(?!amq\.).*'
      nova:
        libvirt:
          images_rbd_pool: {{ .Values.ceph.pools.nova }}
          rbd_user: {{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.cinderVolume }}
          rbd_secret_uuid: {{ .Values.ceph.libvirtUUID }}
        api_database:
          connection_recycle_time: 5
          max_pool_size: 1
          connection_debug: 100
        database:
          connection_recycle_time: 5
          max_pool_size: 1
          #min_pool_size: 0
          connection_debug: 100
          #use_db_reconnect: "True"
          #use_tpool: "True"
        cell0_database:
          connection_recycle_time: 5
          max_pool_size: 1
          connection_debug: 100

- name: placement
  namespace: {{ .Values.openstack.namespace }}
  chart: "{{ requiredEnv "MYDIR" }}/../charts/openstack-helm/placement"
  labels:
    app: openstack
  needs:
  - {{ .Values.openstack.namespace }}/keystone
  #- {{ .Values.openstack.namespace }}/nova
  hooks:
  - *helmToolkitDependencyFixup
  values:
  - *openstackCommon
{{- if .Values.openstack.tls.enabled }}
  - conf:
      placement:
        keystone_authtoken:
          cafile: /etc/placement/certs/ca.crt
    endpoints:
      placement:
        hosts:
          default: placement-api
        scheme:
          default: http
        port:
          api:
            default: 8778
        host_fqdn_override:
          default:
            tls:
              <<: *openstackCAIssuer
              secretName: placement-tls
    secrets:
      tls:
        placement:
          api:
            #public: placement-tls
            public: placement-tls-public
            internal: placement-tls
{{- end }}
  - images:
      tags:
        db_migrate: quay.io/airshipit/porthole-postgresql-utility:latest-{{ .Values.openstack.baseImage }}
        placement: {{ .Values.registry }}/placement:{{ $osTag }}
        placement_db_sync: {{ .Values.registry }}/placement:{{ $osTag }}
    labels:
      api:
        <<: *openstackControlNodeSelector
      job:
        <<: *openstackControlNodeSelector
    network:
      api:
        ingress:
          annotations:
            nginx.org/rewrites: "serviceName=placement-api rewrite=/"
    pod:
      replicas:
        api: 1
    helm3_hook: false

- name: gnocchi
  namespace: {{ .Values.openstack.namespace }}
  chart: "{{ requiredEnv "MYDIR" }}/../charts/openstack-helm-infra/gnocchi"
  installed: false  # gnocchi removed from openstack-helm-infra
  labels:
    app: openstack
  needs:
  - {{ .Values.openstack.namespace }}/patroni
  - {{ .Values.openstack.namespace }}/keystone
  hooks:
  - events: ["presync"]
    command: "/bin/sh"
    args:
    - "-xec"
    - |
      until [ -n "$(kubectl -n {{ .Values.rook.namespace }} get secret/rook-ceph-client-{{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.gnocchi }} -o jsonpath='{.data.{{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.gnocchi }} }' | base64 -dw0)" ]; do sleep 1; done; kubectl -n "{{ .Values.openstack.namespace }}" create secret generic "{{ .Values.ceph.keyringSecrets.gnocchi }}" --type kubernetes.io/rbd --from-literal key="$(kubectl -n {{ .Values.rook.namespace }} get "secret/{{ .Values.openstack.namespace }}-client-{{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.gnocchi }}" -o jsonpath='{.data.{{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.gnocchi }} }' | base64 -dw0)"
  values:
  - *openstackCommon
  - images:
      tags:
        db_sync: {{ .Values.registry }}/gnocchi:{{ $osTag }}
        gnocchi_api: {{ .Values.registry }}/gnocchi:{{ $osTag }}
        gnocchi_statsd: {{ .Values.registry }}/gnocchi:{{ $osTag }}
        gnocchi_metricd: {{ .Values.registry }}/gnocchi:{{ $osTag }}
        gnocchi_resources_cleaner: {{ .Values.registry }}/gnocchi:{{ $osTag }}
        db_init_indexer: docker.io/library/postgres:13
        gnocchi_storage_init: {{ .Values.registry }}/ceph-config-helper:{{ $osTag }}
    labels:
      api:
        <<: *openstackControlNodeSelector
      job:
        <<: *openstackControlNodeSelector
      metricd:
        <<: *openstackControlNodeSelector
      statsd:
        <<: *openstackControlNodeSelector
    network:
      api:
        ingress:
          annotations:
            nginx.org/rewrites: "serviceName=gnocchi-api rewrite=/"
    ceph_client:
      user_secret_name: {{ .Values.ceph.keyringSecrets.admin }}
    secrets:
      rbd: {{ .Values.ceph.keyringSecrets.gnocchi }}
      tls:
        metric:
          api:
            #public: gnocchi-tls
            public: gnocchi-tls-public
    endpoints:
      metric:
        hosts:
          default: gnocchi-api
        scheme:
          default: http
        port:
          api:
            default: 8041
        host_fqdn_override:
          default:
            tls:
              <<: *openstackCAIssuer
              secretName: gnocchi-tls
      oslo_db_postgresql:
        hosts:
          default: {{ $.Values.credentials.database.root.host | quote }}
        scheme: {{ $.Values.credentials.database.root.scheme | quote }}
        port:
          postgresql:
            default: {{ $.Values.credentials.database.root.port }}
        path: /gnocchi_indexer
        auth:
          gnocchi:
            username: {{ $.Values.credentials.database.gnocchi.user | quote }}
            password: {{ $.Values.credentials.database.gnocchi.pass | quote }}
          admin:
            username: {{ $.Values.credentials.database.root.user | quote }}
            password: {{ $.Values.credentials.database.root.pass | quote }}
            secret:
              tls:
{{- if .Values.openstack.tls.enabled }}
                internal: {{ .Values.openstack.tls.secrets.database }}
{{- else }}
                internal: ""
{{- end }}
    conf:
      gnocchi:
        storage:
          ceph_pool: {{ .Values.ceph.pools.gnocchi }}
          ceph_username: {{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.gnocchi }}
          ceph_keyring: "/etc/ceph/ceph.client.{{ .Values.openstack.namespace }}-{{ .Values.ceph.keyringSecrets.gnocchi }}.keyring"
      paste:
        composite:gnocchi+keystone:
          use: egg:Paste#urlmap
          /: gnocchiversions
          /v1: gnocchiv1+auth
        app:gnocchiversions:
          root: gnocchi.rest.api.VersionsController
        app:gnocchiv1:
          root: gnocchi.rest.api.V1Controller
        #pipeline:gnocchiv1+keystone:
        #  pipeline: keystone_authtoken gnocchiv1
    helm3_hook: false

- name: ceilometer
  namespace: {{ .Values.openstack.namespace }}
  chart: "{{ requiredEnv "MYDIR" }}/../charts/openstack-helm/ceilometer"
  installed: false
  labels:
    app: openstack
  needs:
  - {{ .Values.openstack.namespace }}/keystone
  - {{ .Values.openstack.namespace }}/gnocchi
  hooks:
  - *helmToolkitDependencyFixup
  values:
  - *openstackCommon
  - images:
      tags:
        ceilometer_db_sync: {{ .Values.registry }}/ceilometer:{{ $osTag }}
        ceilometer_api: {{ .Values.registry }}/ceilometer:{{ $osTag }}
        ceilometer_central: {{ .Values.registry }}/ceilometer:{{ $osTag }}
        ceilometer_collector: {{ .Values.registry }}/ceilometer:{{ $osTag }}
        ceilometer_compute: {{ .Values.registry }}/ceilometer:{{ $osTag }}
        ceilometer_ipmi: {{ .Values.registry }}/ceilometer:{{ $osTag }}
        ceilometer_notification: {{ .Values.registry }}/ceilometer:{{ $osTag }}
        #db_init_mongodb: docker.io/mongo:4.4.6-bionic
    labels:
      api:
        <<: *openstackControlNodeSelector
      central:
        <<: *openstackControlNodeSelector
      collector:
        <<: *openstackControlNodeSelector
      notification:
        <<: *openstackControlNodeSelector
      job:
        <<: *openstackControlNodeSelector
      compute:
        <<: *openstackComputeNodeSelector
      ipmi:
        <<: *openstackBaremetalNodeSelector
    network:
      api:
        ingress:
          annotations:
            nginx.org/rewrites: "serviceName=ceilometer-api rewrite=/"
    helm3_hook: false
    conf:
      ceilometer:
        DEFAULT:
          debug: "True"
        api:
          aodh_is_enabled: "True"
          gnocchi_is_enabled: "False"
        database:
          metering_connection: "{{ $.Values.credentials.database.root.scheme }}://{{ $.Values.credentials.database.ceilometer.user }}:{{ $.Values.credentials.database.ceilometer.pass }}@{{ $.Values.credentials.database.root.host }}:{{ $.Values.credentials.database.root.port }}/ceilometer"
          event_connection: "{{ $.Values.credentials.database.root.scheme }}://{{ $.Values.credentials.database.ceilometer.user }}:{{ $.Values.credentials.database.ceilometer.pass }}@{{ $.Values.credentials.database.root.host }}:{{ $.Values.credentials.database.root.port }}/ceilometer"
        notification:
          messaging_urls:
            type: multistring
            values:
{{- range (list "keystone" "glance" "cinder" "heat" "nova" "neutron" "barbican" "octavia" "designate") }}
            - "rabbit://{{ $.Values.credentials.rabbitmq.root.user }}:{{ $.Values.credentials.rabbitmq.root.pass }}@{{ $.Values.credentials.rabbitmq.root.host }}:5672/{{ . }}"
{{- end }}
      pipeline:
        sinks:
        - name: meter_sink
          transformers:
          publishers:
          - notifier://
          #- gnocchi://?archive_policy=low
        - name: cpu_sink
          transformers:
          - name: "rate_of_change"
            parameters:
              target:
                name: "cpu_util"
                unit: "%"
                type: "gauge"
                max: 100
                scale: "100.0 / (10**9 * (resource_metadata.cpu_number or 1))"
          publishers:
          - notifier://
          #- gnocchi://?archive_policy=low
        - name: cpu_delta_sink
          transformers:
          - name: "delta"
            parameters:
              target:
                name: "cpu.delta"
              growth_only: True
          publishers:
          - notifier://
          #- gnocchi://?archive_policy=low
        - name: disk_sink
          transformers:
          - name: "rate_of_change"
            parameters:
              source:
                map_from:
                  name: "(disk\\.device|disk)\\.(read|write)\\.(bytes|requests)"
                  unit: "(B|request)"
              target:
                map_to:
                  name: "\\1.\\2.\\3.rate"
                  unit: "\\1/s"
                type: "gauge"
          publishers:
          - notifier://
          #- gnocchi://?archive_policy=low
        - name: network_sink
          transformers:
          - name: "rate_of_change"
            parameters:
              source:
                map_from:
                  name: "network\\.(incoming|outgoing)\\.(bytes|packets)"
                  unit: "(B|packet)"
              target:
                map_to:
                  name: "network.\\1.\\2.rate"
                  unit: "\\1/s"
                type: "gauge"
          publishers:
          - notifier://
          #- gnocchi://?archive_policy=low
    manifests:
      deployment_api: false
      deployment_collector: false
      job_db_init_mongodb: false
      secret_mongodb: false
    dependencies:
      static:
        db_sync:
          jobs:
          - ceilometer-db-init
          services:
          - endpoint: internal
            service: oslo_db
{{- range (list "api" "central" "ipmi" "collector" "compute" "notification") }}
        {{ . }}:
          jobs:
          - ceilometer-db-sync
          - ceilometer-rabbit-init
          - ceilometer-ks-user
          - ceilometer-ks-endpoints
          services:
          - endpoint: internal
            service: identity
          - endpoint: internal
            service: oslo_db
          - endpoint: internal
            service: metric
{{- end }}

- name: aodh
  namespace: {{ .Values.openstack.namespace }}
  chart: "{{ requiredEnv "MYDIR" }}/../charts/openstack-helm/aodh"
  #installed: false
  labels:
    app: openstack
  needs:
  - {{ .Values.openstack.namespace }}/keystone
  - {{ .Values.openstack.namespace }}/ceilometer
  hooks:
  - *helmToolkitDependencyFixup
  values:
  - *openstackCommon
  - endpoints:
      oslo_db:
        scheme: postgresql
    images:
      tags:
        aodh_db_sync: {{ .Values.registry }}/aodh:{{ $osTag }}
        aodh_api: {{ .Values.registry }}/aodh:{{ $osTag }}
        aodh_evaluator: {{ .Values.registry }}/aodh:{{ $osTag }}
        aodh_listener: {{ .Values.registry }}/aodh:{{ $osTag }}
        aodh_notifier: {{ .Values.registry }}/aodh:{{ $osTag }}
        aodh_alarms_cleaner: {{ .Values.registry }}/aodh:{{ $osTag }}
    labels:
      api:
        <<: *openstackControlNodeSelector
      evaluator:
        <<: *openstackControlNodeSelector
      listener:
        <<: *openstackControlNodeSelector
      notifier:
        <<: *openstackControlNodeSelector
      job:
        <<: *openstackControlNodeSelector
    network:
      api:
        ingress:
          annotations:
            nginx.org/rewrites: "serviceName=aodh-api rewrite=/"
    helm3_hook: false

- name: barbican
  namespace: {{ .Values.openstack.namespace }}
  chart: "{{ requiredEnv "MYDIR" }}/../charts/openstack-helm/barbican"
  installed: false  # add tls support in job-ks-service.yaml
  labels:
    app: openstack
  needs:
  - {{ .Values.openstack.namespace }}/keystone
  - {{ .Values.openstack.namespace }}/neutron
  hooks:
  - *helmToolkitDependencyFixup
  values:
  - *openstackCommon
{{- if .Values.openstack.tls.enabled }}
  - conf:
      barbican:
        keystone_authtoken:
          cafile: /etc/barbican/certs/ca.crt
    endpoints:
      key_manager:
        hosts:
          default: barbican-api
        scheme:
          default: http
        port:
          api:
            default: 9311
        host_fqdn_override:
          default:
            tls:
              <<: *openstackCAIssuer
              secretName: barbican-tls
    secrets:
      tls:
        key_manager:
          api:
            #public: barbican-tls
            public: barbican-tls-public
{{- end }}
  - images:
      tags:
        barbican_db_sync: {{ .Values.registry }}/barbican:{{ $osTag }}
        barbican_api: {{ .Values.registry }}/barbican:{{ $osTag }}
    labels:
      api:
        <<: *openstackControlNodeSelector
      job:
        <<: *openstackControlNodeSelector
    network:
      api:
        ingress:
          annotations:
            nginx.org/rewrites: "serviceName=barbican-api rewrite=/"
    pod:
      replicas:
        api: 1
    helm3_hook: false

- name: octavia
  namespace: {{ .Values.openstack.namespace }}
  chart: "{{ requiredEnv "MYDIR" }}/../charts/openstack-helm/octavia"
  #installed: false  # add tls support in job-ks-service.yaml
  labels:
    app: openstack
  needs:
  - {{ .Values.openstack.namespace }}/keystone
  - {{ .Values.openstack.namespace }}/neutron
  hooks:
  - *helmToolkitDependencyFixup
  values:
  - *openstackCommon
{{- if .Values.openstack.tls.enabled }}
  - conf:
      octavia:
        keystone_authtoken:
          cafile: /etc/octavia/certs/ca.crt
        controller_worker:  # https://docs.openstack.org/octavia/latest/install/install.html
          amp_image_tag: {{ $.Values.openstack.octavia.amphora.image_tag | quote }}
{{/*
          amp_boot_network_list: ??
          amp_flavor_id: ??
          amp_image_owner_id: ??
          amp_secgroup_list: ??
          amp_ssh_key_name: octavia_ssh_key
          amp_active_retries: 100
          amp_active_wait_sec: 2
          network_driver: allowed_address_pairs_driver
          compute_driver: compute_nova_driver
          amphora_driver: amphora_haproxy_rest_driver
*/}}
    endpoints:
      load_balancer:
        hosts:
          default: octavia-api
        scheme:
          default: http
        port:
          api:
            default: 9876
        host_fqdn_override:
          default:
            tls:
              <<: *openstackCAIssuer
              secretName: octavia-tls
    secrets:
      tls:
        load_balancer:
          api:
            #public: octavia-tls
            public: octavia-tls-public
{{- end }}
  - images:
      tags:
        octavia_db_sync: {{ .Values.registry }}/octavia:{{ $osTag }}
        octavia_api: {{ .Values.registry }}/octavia:{{ $osTag }}
        octavia_worker: {{ .Values.registry }}/octavia:{{ $osTag }}
        octavia_housekeeping: {{ .Values.registry }}/octavia:{{ $osTag }}
        octavia_health_manager: {{ .Values.registry }}/octavia:{{ $osTag }}
        octavia_health_manager_init: docker.io/kolla/ubuntu-source-octavia-health-manager:{{ .Values.openstack.version }}
        openvswitch_vswitchd: docker.io/kolla/centos-source-openvswitch-vswitchd:{{ .Values.openstack.version }}
    labels:
      api:
        <<: *openstackControlNodeSelector
      health_manager:
        <<: *openstackControlNodeSelector
      housekeeping:
        <<: *openstackControlNodeSelector
      job:
        <<: *openstackControlNodeSelector
      worker:
        <<: *openstackControlNodeSelector
    network:
      api:
        ingress:
          annotations:
            nginx.org/rewrites: "serviceName=octavia-api rewrite=/"
    pod:
      replicas:
        api: 1
        worker: 1
        housekeeping: 1
    manifests:
      daemonset_health_manager: false
    helm3_hook: false

- name: designate
  namespace: {{ .Values.openstack.namespace }}
  chart: "{{ requiredEnv "MYDIR" }}/../charts/openstack-helm/designate"
  #installed: false  # add tls support in job-ks-service.yaml
  labels:
    app: openstack
  needs:
  - {{ .Values.openstack.namespace }}/keystone
  - {{ .Values.openstack.namespace }}/powerdns
  hooks:
  - *helmToolkitDependencyFixup
  values:
  - *openstackCommon
{{- if .Values.openstack.tls.enabled }}
  - conf:
      designate:
        keystone_authtoken:
          cafile: /etc/designate/certs/ca.crt
    endpoints:
      dns:
        hosts:
          default: designate-api
        scheme:
          default: http
        port:
          api:
            default: 9001
        host_fqdn_override:
          default:
            tls:
              <<: *openstackCAIssuer
              secretName: designate-tls
    secrets:
      tls:
        dns:
          api:
            #public: designate-tls
            public: designate-tls-public
{{- end }}
  - images:
      tags:
        designate_api: {{ .Values.registry }}/designate:{{ $osTag }}
        designate_central: {{ .Values.registry }}/designate:{{ $osTag }}
        designate_db_sync: {{ .Values.registry }}/designate:{{ $osTag }}
        designate_mdns: {{ .Values.registry }}/designate:{{ $osTag }}
        designate_producer: {{ .Values.registry }}/designate:{{ $osTag }}
        designate_sink: {{ .Values.registry }}/designate:{{ $osTag }}
        designate_worker: {{ .Values.registry }}/designate:{{ $osTag }}
    labels:
      api:
        <<: *openstackControlNodeSelector
      central:
        <<: *openstackControlNodeSelector
      producer:
        <<: *openstackControlNodeSelector
      worker:
        <<: *openstackControlNodeSelector
      job:
        <<: *openstackControlNodeSelector
      mdns:
        <<: *openstackControlNodeSelector
      sink:
        <<: *openstackControlNodeSelector
    network:
      api:
        ingress:
          annotations:
            nginx.org/rewrites: "serviceName=designate-api rewrite=/"
    pod:
      replicas:
        api: 1
        central: 1
        mdns: 1
        producer: 1
        sink: 1
        worker: 1
    helm3_hook: false

- name: magnum
  namespace: {{ .Values.openstack.namespace }}
  chart: "{{ requiredEnv "MYDIR" }}/../charts/openstack-helm/magnum"
  installed: false
  labels:
    app: openstack
  needs:
  - {{ .Values.openstack.namespace }}/keystone
  hooks:
  - *helmToolkitDependencyFixup
  values:
  - *openstackCommon
  - images:
      tags:
        magnum_api: {{ .Values.registry }}/magnum:{{ $osTag }}
        magnum_conductor: {{ .Values.registry }}/magnum:{{ $osTag }}
        magnum_db_sync: {{ .Values.registry }}/magnum:{{ $osTag }}
    labels:
      api:
        <<: *openstackControlNodeSelector
      conductor:
        <<: *openstackControlNodeSelector
      job:
        <<: *openstackControlNodeSelector
    helm3_hook: false

- name: senlin
  namespace: {{ .Values.openstack.namespace }}
  chart: "{{ requiredEnv "MYDIR" }}/../charts/openstack-helm/senlin"
  installed: false
  labels:
    app: openstack
  needs:
  - {{ .Values.openstack.namespace }}/keystone
  hooks:
  - *helmToolkitDependencyFixup
  values:
  - *openstackCommon
  - images:
      tags:
        senlin_api: {{ .Values.registry }}/magnum:{{ $osTag }}
        senlin_db_sync: {{ .Values.registry }}/magnum:{{ $osTag }}
        senlin_engine: {{ .Values.registry }}/magnum:{{ $osTag }}
        senlin_engine_cleaner: {{ .Values.registry }}/magnum:{{ $osTag }}
    labels:
      api:
        <<: *openstackControlNodeSelector
      engine:
        <<: *openstackControlNodeSelector
      job:
        <<: *openstackControlNodeSelector
    network:
      dashboard:
        ingress:
          annotations:
            nginx.org/rewrites: "serviceName=senlin-api rewrite=/"
    helm3_hook: false

# would love to do manila and trove helm charts some day

- name: horizon
  namespace: {{ .Values.openstack.namespace }}
  chart: "{{ requiredEnv "MYDIR" }}/../charts/openstack-helm/horizon"
  labels:
    app: openstack
  needs:
  - {{ .Values.openstack.namespace }}/keystone
  hooks:
  - *helmToolkitDependencyFixup
  values:
  - *openstackCommon
{{- if .Values.openstack.tls.enabled }}
  - endpoints:
      dashboard:
        hosts:
          default: horizon-int
        scheme:
          default: http
        port:
          web:
            default: 80
        host_fqdn_override:
          default:
            tls:
              <<: *openstackCAIssuer
              secretName: horizon-tls
    secrets:
      tls:
        dashboard:
          dashboard:
            #public: horizon-tls
            public: horizon-tls-public
            internal: horizon-tls
{{- end }}
  - images:
      tags:
        horizon_db_sync: {{ .Values.registry }}/horizon:{{ $osTag }}
        horizon: {{ .Values.registry }}/horizon:{{ $osTag }}
    labels:
      dashboard:
        <<: *openstackControlNodeSelector
      job:
        <<: *openstackControlNodeSelector
    network:
      dashboard:
        ingress:
          annotations:
            nginx.org/rewrites: "serviceName=horizon-int rewrite=/"
    pod:
      replicas:
        server: 1
    helm3_hook: false
